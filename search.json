[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mis Proyectos",
    "section": "",
    "text": "1 Prólogo\nEn este libro se recogen la mayoría de trabajos que he realizado en el ámbito de la ciencia de datos y la estadística.\nLa mayoría de estos trabajos han sido realizados en las diversas asignaturas del Máster en Matemáticas salvo el TFG que fue realizado el último curso del Grado en Matemáticas. Todos estos trabajos los he realizado en la Universidad de Valladolid a lo largo del 2023 y del 2024.\nGran parte de estos trabajos han sido resueltos en R salvo los que así estén indicados. El código de estos otros trabajos se encuentra también en el archivo comprimido."
  },
  {
    "objectID": "Eda.html",
    "href": "Eda.html",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "",
    "text": "3 Resumen Ejecutivo\nDesde los inicios de nuestra especie hemos observado el firmamento. Nuestra innata curiosidad buscaba respuestas a lo que podíamos ver y hasta recientemente en la historia no hemos sido capaces de responder con seguridad. Aún así, hay muchas preguntas aún sin respuesta y por ello seguimos explorando el universo en el que vivimos. ¿Cúal es nuestro origen?¿Estamos solos?¿Podemos habitar otro planeta?\nCon el objetivo de responder a estas preguntas lanzamos satélites y sondas desde el siglo anterior. Hasta lanzar el observatorio espacial Kepler el 7 de marzo de 2009 la cifra de exoplanetas conocidos era inferior a la que Kepler eventualmente contribuyó a descubrir.\nEl Observatorio Espacial Kepler, lanzado en 2009, jugó un papel fundamental en el aumento significativo del número de exoplanetas conocidos. Utilizando el método de tránsito, Kepler observó la disminución en el brillo de las estrellas cuando un planeta pasaba frente a ellas, lo que permitió identificar y confirmar numerosos exoplanetas.\nEl telescopio Kepler proporcionó datos valiosos para la misión de búsqueda de exoplanetas, y su sucesor, el Telescopio Espacial TESS (Transiting Exoplanet Survey Satellite), lanzado en 2018, continuó esta tarea al identificar exoplanetas adicionales en diferentes regiones del cielo.\nEn este análisis de datos estudiaremos los datos que recogió dicho observatorio a lo largo de casi 10 años de misión hasta que vació sus reservas de combustible. Estudiaremos las variables que medía e intentaremos descubrir alguna forma para predecir si una observación es un exoplaneta o no.\nEn primer lugar, cargamos el conjunto de datos y mostramos sus primeras filas\nComenzaremos viendo si dichas variables son necesarias para el estudio de nuestro conjunto de datos. Para ello convertimos la varible koi_disposition que es la que nos muestra la disposición de un exoplaneta, es decir, si es un exoplaneta un candidato o un falso positivo detectado por el telescopio.\ndatosshow &lt;- datos[, -c(1,2,3,5,30,31,37)]\ndatosshow$koi_disposition &lt;- ifelse(datosshow$koi_disposition == \"CONFIRMED\", 1,\n                                ifelse(datosshow$koi_disposition == \"CANDIDATE\", 0, 2))\ndatosshow&lt;-na.omit(datosshow)\ncorrelation_matrix &lt;- cor(datosshow)\ncorrplot(correlation_matrix, tl.cex = 0.6)\nPodemos observar que los errores no tienen correlación alta con las variables que miden las disposiciones de las observaciones y por tanto trabajaremos con el conjunto de datos sin ellos.\ncolumnas_a_mantener &lt;- colnames(datos)[!grepl(\"err\", colnames(datos))]\ndatos &lt;- datos[, columnas_a_mantener]\nAhora nuestro conjunto de datos se reduce a 27 variables.\nAntes de analizar cada tipo de atributo analizaremos los datos no disponibles o NAs de nuestro conjunto de datos completo. A continuación mostramos cuántos datos faltantes tienen nuestras variables:\nvalores_faltantes &lt;- colSums(is.na(datos))\nprint(valores_faltantes)\n\n            kepid        kepoi_name       kepler_name   koi_disposition \n                0                 0                 0                 0 \n koi_pdisposition         koi_score     koi_fpflag_nt     koi_fpflag_ss \n                0              1510                 0                 0 \n    koi_fpflag_co     koi_fpflag_ec        koi_period       koi_time0bk \n                0                 0                 0                 0 \n       koi_impact      koi_duration         koi_depth          koi_prad \n              363                 0               363               363 \n          koi_teq         koi_insol     koi_model_snr  koi_tce_plnt_num \n              363               321               363               346 \nkoi_tce_delivname         koi_steff         koi_slogg          koi_srad \n                0               363               363               363 \n               ra               dec        koi_kepmag \n                0                 0                 1\nEl siguiente gráfico nos indica qué variables tienen mayor número de NAs.\nrojo &lt;- rgb(0.925, 0.424, 0.392)\nporcentaje_na &lt;- valores_faltantes / nrow(datos)\ndf_porcentaje_na &lt;- data.frame(variable = names(porcentaje_na), porcentaje = porcentaje_na)\ndf_porcentaje_na &lt;- data.frame(variable = names(porcentaje_na), porcentaje = porcentaje_na) %&gt;%\n  filter(porcentaje &gt; 0) %&gt;%\n  arrange(desc(porcentaje))\n\nsuppressWarnings({\nggplot(df_porcentaje_na, aes(x = reorder(variable, -porcentaje), y = porcentaje)) +\n  geom_bar(stat = \"identity\", fill = rojo) +\n  labs(title = \"Porcentaje de NA por Variable\",\n       x = \"Variable\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        panel.grid = element_blank(),\n        panel.background = element_rect(colour = \"black\", size = 2),\n        plot.title = element_text(hjust = 0.5))+\n  theme(plot.margin = margin(l = 30, r = 10, b = 10, t = 10, unit = \"pt\"))\n})\nTras ver el número de NAs de las variables resulta curioso que algunas tengan exactamente el mismo número, por ello vamos a comprobar si dichos valores ausentes son en las mismas observaciones.\nNASkoi_impact &lt;- datos[is.na(datos$koi_impact), ]\nNASkoi_depth &lt;- datos[is.na(datos$koi_depth), ]\nNASkoi_prad &lt;- datos[is.na(datos$koi_prad), ]\nNASkoi_depth &lt;- datos[is.na(datos$koi_depth), ]\nNASkoi_teq &lt;- datos[is.na(datos$koi_teq), ]\nNASkoi_model_snr &lt;- datos[is.na(datos$koi_model_snr), ]\nNASkoi_steff &lt;- datos[is.na(datos$koi_steff), ]\nNASkoi_slogg &lt;- datos[is.na(datos$koi_slogg), ]\nNASkoi_srad &lt;- datos[is.na(datos$koi_srad), ]\n\nnas_data_frames &lt;- list(NASkoi_impact, NASkoi_depth, NASkoi_prad, NASkoi_depth, NASkoi_teq, NASkoi_model_snr, NASkoi_steff, NASkoi_slogg, NASkoi_srad)\n\nresultados &lt;- sapply(2:length(nas_data_frames), function(i) identical(nas_data_frames[[1]], nas_data_frames[[i]]))\n\nresultados\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nComo vemos todos esos NAS son de las mismas observaciones, veamos la frecuencia de su disposición.\ndisposicionplaneta363NA&lt;-NASkoi_impact$koi_disposition\nfrecuencia363NA&lt;-table(disposicionplaneta363NA)\nprint(frecuencia363NA)\n\ndisposicionplaneta363NA\n     CANDIDATE      CONFIRMED FALSE POSITIVE \n           104              2            257\ndisposicionplaneta363NA &lt;- c(CANDIDATE = 104, CONFIRMED = 2, `FALSE POSITIVE` = 257)\nbarplot(disposicionplaneta363NA, col = rojo,\n        main = \"Gráfico de Barras de Frecuencias\",\n        xlab = \"Disposición de las observaciones\",\n        ylab = \"Frecuencia\")\nAlrededor del 70% de los NAs que disponemos son falsos positivos lo que nos indica que si el telescopio no ha podido recopilar dichos datos es porque es bastante probable que no haya observado un exoplaneta.\nAhora vamos a estudiar los otros 3 data frames. Empezaremos por los que no disponen de “Número de Planeta TCE” que es un identificador utilizado en el marco del proyecto Kepler para numerar y seguir los eventos que podrían ser causados por la presencia de exoplanetas.\nNASkoi_tce_plnt_num &lt;- datos[is.na(datos$koi_tce_plnt_num), ]\ndisposicionplanetakoi_tce_plnt_num&lt;-NASkoi_tce_plnt_num$koi_disposition\nfrecuenciakoi_tce_plnt_num&lt;-table(disposicionplanetakoi_tce_plnt_num)\nprint(frecuenciakoi_tce_plnt_num)\n\ndisposicionplanetakoi_tce_plnt_num\n     CANDIDATE      CONFIRMED FALSE POSITIVE \n            60             11            275\nEn este caso observamos que el porcenttaje de falsos positivos es mucho mayor. Seguimos estudiando el Flujo de Insolación en flujo terrestre que es una medida que proporciona una forma de comparar la energía solar recibida por un objeto celeste.\nNASkoi_koi_insol &lt;- datos[is.na(datos$koi_insol), ]\ndisposicionplanetakoi_insol&lt;-NASkoi_koi_insol$koi_disposition\nfrecuenciakoi_insol&lt;-table(disposicionplanetakoi_insol)\nprint(frecuenciakoi_insol)\n\ndisposicionplanetakoi_insol\n     CANDIDATE      CONFIRMED FALSE POSITIVE \n           102              1            218\nAl igual que en casos anteriores 2 tercias partes de dichas observaciones son falsos positivos.\nPor último estudiaremos el atributo que mayor porcentaje de NAs contiene. Además es el más importante pues es el que más correlacionado está con la clase koi_disposition.\nNASkoi_score &lt;- datos[is.na(datos$koi_score), ]\ndisposicionplanetakoi_score&lt;-NASkoi_score$koi_disposition\nfrecuenciakoi_score&lt;-table(disposicionplanetakoi_score)\nprint(frecuenciakoi_score)\n\ndisposicionplanetakoi_score\n     CANDIDATE      CONFIRMED FALSE POSITIVE \n           600             17            893\nEn este caso el porcentaje es un poco menor alrededor de \\(\\frac{3}{5}\\) en vez \\(\\frac{2}{3}\\). Posteriormente estudiaremos más a fondo esta variable.\nEn este apartado intentaremos agrupar las observaciones usando clustering para ver cuántas similares obtenemos y ver si podemos dividirlas en las que son falsos positivos y candidatos a exoplanetas.\ndatossinna&lt;-na.omit(datos)\ndatossinna&lt;-datossinna[,-c(1,2,3,4,5,20,21)]\nVamos a realizar una transformación a escala logarítmica para que nuestro modelo funcione mejor.\ndatossinna &lt;- datossinna + 0.01\nlogscaled_data &lt;- log(datossinna)\nA continuación probaremos dos tipos de escalados a ver cual recoge más información.\ndatosscale &lt;- scale(logscaled_data)\n\nmin_max_scaler &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ndatosminmax &lt;- apply(logscaled_data, 2, min_max_scaler)\nAhora analizamos el que resume mejor las variables usando prcomp. En primer lugar usando scale.\npr.out.scale &lt;- prcomp(datosscale, scale = FALSE)\npr.var.scale &lt;- pr.out.scale$sdev^2\npve.scale &lt;- pr.var.scale / sum(pr.var.scale) \npve.scale*100\n\n [1] 22.24792518722 17.75209278272 11.69848637517  9.68824979665  5.68847219376\n [6]  4.92223378770  4.87099400644  4.59746167552  4.24546721542  3.27239140593\n[11]  3.10305252741  2.38482151231  1.81918396033  1.52317417005  0.88254224109\n[16]  0.66325154410  0.37740402406  0.26051723050  0.00225436173  0.00002400189\nAhora veamos usando minmax.\npr.out.minmax &lt;- prcomp(datosminmax, scale = FALSE)\npr.var.minmax &lt;- pr.out.minmax$sdev^2\npve.minmax &lt;- pr.var.minmax / sum(pr.var.minmax) \npve.minmax*100\n\n [1] 42.511671731140 20.379104094109  6.897720278308  6.278672501709\n [5]  5.581966113416  5.018029200126  4.387472164020  2.205890771574\n [9]  1.783499804493  1.560280542892  1.037085356186  0.636873835729\n[13]  0.574370139539  0.533762489543  0.353699642688  0.156643850651\n[17]  0.086043623657  0.016412935711  0.000793263919  0.000007660591\nDibujamos los datos en las 3 primeras componentes principales\nset.seed(1234)\n\nindices_aleatorios &lt;- sample(1:nrow(pr.out.minmax$x), size = nrow(pr.out.minmax$x) / 2)\n\npca_data &lt;- pr.out.minmax$x[indices_aleatorios, 1:3]\n\n\n#plot3d(pca_data, col = rojo, type = \"s\", radius = 0.1)\n\nscatter3D(pca_data[, 1], pca_data[, 2], pca_data[, 3], col = rojo, pch = 16,\n          xlab = \"Componente 1\", ylab = \"Componente 2\", zlab = \"Componente 3\")\nAhora realizaremos un clustering para detectar el número de clusters.\nclus &lt;- hclust(dist(datosminmax), method = \"ward.D2\")\nplot(clus)\nA continuación probaremos a hacer un modelo que nos prediga si nuestras observaciones serán exoplanetas o no. Para ello separaremos los datos sin NA en 3 grupos, los que todavía no están etiquetados, los de prueba y los de test.\ndatossinna&lt;-na.omit(datos)\n\ndatoscandidatos &lt;- datossinna[datossinna$koi_disposition==\"CANDIDATE\",]\ndatosmodelo &lt;- datossinna[datossinna$koi_disposition!=\"CANDIDATE\",]\n\nsplit &lt;- sample.split(datosmodelo$koi_disposition, SplitRatio = 0.70)\ndatosentrenamiento &lt;- subset(datosmodelo, split == TRUE)\ndatostest &lt;- subset(datosmodelo, split == FALSE)\nUna vez seleccionados los datos eliminamos las variables que no vamos a utilizar en los tres conjuntos.\ndatoscandidatos &lt;- datoscandidatos[,-c(1,2,3,5,20,21)]\ndatosentrenamiento &lt;- datosentrenamiento[,-c(1,2,3,5,20,21)]\ndatostest &lt;- datostest[,-c(1,2,3,5,20,21)]\nAhora modificamos la etiqueta para que sea un valor numérico.\ndatosentrenamiento$koi_disposition &lt;- ifelse(datosentrenamiento$koi_disposition == \"CONFIRMED\", 1, 0)\ndatostest$koi_disposition &lt;- ifelse(datostest$koi_disposition == \"CONFIRMED\", 1, 0)\nAhora entrenamos por validación cruzada un modelo de regresión logística\nsuppressMessages({\n         suppressWarnings({\nfolds &lt;- createFolds(datosentrenamiento$koi_disposition, k = 5)\n\nlistamodelos &lt;- list()\nlistaprecisiones &lt;- list()\n\nfor (i in 1:5) {\n  training_fold &lt;- datosentrenamiento[-folds[[i]],]\n  test_fold &lt;- datosentrenamiento[folds[[i]],]\n  clasificador &lt;- glm(koi_disposition ~ ., family = binomial, data = training_fold)\n  y_pred &lt;- predict(clasificador, type = 'response', newdata = test_fold)\n  y_pred &lt;- ifelse(y_pred &gt; 0.5, 1, 0)\n  y_pred &lt;- factor(y_pred, levels = c(\"0\", \"1\"), labels = c(\"MM\", \"CH\"))\n  mc &lt;- table(test_fold$koi_disposition, y_pred)\n  precision &lt;- (mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\n  listamodelos[[i]] &lt;- clasificador\n  listaprecisiones[[i]] &lt;- precision\n}\nprecisionRegresionLogistica &lt;- mean(as.numeric(listaprecisiones))\ncat(\"\\nPrecision Media Validación Cruzada Regresion Logistica:\",precisionRegresionLogistica,\"\\n\")\ncat(\"\\nError Medio Validación Cruzada Regresion Logistica:\",1-precisionRegresionLogistica,\"\\n\")\n\nindice&lt;- which.max(listaprecisiones)\nmejormodeloregresionlogistica &lt;- listamodelos[[indice]]\npredicciones &lt;- predict(mejormodeloregresionlogistica, type = 'response', newdata = datostest)\npredicciones &lt;- ifelse(predicciones &gt; 0.5, 1, 0)\npredicciones &lt;- factor(predicciones, levels = c(\"0\", \"1\"), labels = c(\"FALSE POSITIVE\", \"CONFIRMED\"))\nmc &lt;- table(datostest$koi_disposition, predicciones)\nprecision &lt;-(mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\ncat(\"\\nPrecision Regresion Logistica:\",precision,\"\\n\")\ncat(\"\\nError Regresion Logistica:\",1-precision,\"\\n\")\nprint(mc)\n}) })\n\n\nPrecision Media Validación Cruzada Regresion Logistica: 0.9898528 \n\nError Medio Validación Cruzada Regresion Logistica: 0.01014723 \n\nPrecision Regresion Logistica: 0.9984887 \n\nError Regresion Logistica: 0.001511335 \n   predicciones\n    FALSE POSITIVE CONFIRMED\n  0           1167         1\n  1              2       815\nEntrenamos otro modelo para confirmar los resultados.\nsuppressMessages({\n         suppressWarnings({\nlibrary(randomForest)\n\nlistamodelos &lt;- list()\nlistaprecisiones &lt;- list()\n\nfor (i in 1:5) {\n  training_fold &lt;- datosentrenamiento[-folds[[i]], ]\n  test_fold &lt;- datosentrenamiento[folds[[i]], ]\n  training_fold$koi_disposition &lt;- as.factor(training_fold$koi_disposition)\n  test_fold$koi_disposition &lt;- as.factor(test_fold$koi_disposition)\n  clasificador &lt;- randomForest(koi_disposition ~ ., data = training_fold, ntree = 1000)\n  y_pred &lt;- predict(clasificador, newdata = test_fold)\n  mc &lt;- table(test_fold$koi_disposition, y_pred)\n  precision &lt;- (mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\n  listamodelos[[i]] &lt;- clasificador\n  listaprecisiones[[i]] &lt;- precision\n}\nprecisionRandomForest &lt;- mean(as.numeric(listaprecisiones))\n\ncat(\"\\nPrecision media RandomForest:\",precisionRandomForest,\"\\n\")\ncat(\"\\nError medio RandomForest:\",1-precisionRandomForest,\"\\n\")\n\nmejormodelorandomforest &lt;- listamodelos[[indice]]\ndatostest$koi_disposition &lt;- as.factor(datostest$koi_disposition)\npredicciones &lt;- predict(mejormodelorandomforest, newdata = datostest, type = 'class')\nmc &lt;- table(datostest$koi_disposition, predicciones)\n\nprecision &lt;-(mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\ncat(\"\\nPrecision RandomForest:\",precision,\"\\n\")\ncat(\"\\nError RandomForest:\",1-precision,\"\\n\")\nprint(mc)\n\n}) })\n\n\nPrecision media RandomForest: 0.9935224 \n\nError medio RandomForest: 0.006477618 \n\nPrecision RandomForest: 0.9899244 \n\nError RandomForest: 0.01007557 \n   predicciones\n       0    1\n  0 1167    1\n  1   19  798\nFinalizaremos el trabajo prediciendo los candidatos que todavía no se han confirmado. Usaremos los dos modelos a ver qué obtenemos.\nfeatures &lt;- datoscandidatos[, -1]\n\npredicciones_logisticas &lt;- predict(mejormodeloregresionlogistica, newdata = features, type = \"response\")\npredicciones_randomforest &lt;- predict(mejormodelorandomforest, newdata = features, type = \"response\")\n\npredicciones_logisticas &lt;- ifelse(predicciones_logisticas &gt; 0.5, 1, 0)\nCon las siguientes tablas observamos que de dichos candidatos la mayoría o casi todos en el caso de la regresión logística los categorizamos como exoplanetas.\ntable(predicciones_logisticas)\n\npredicciones_logisticas\n   0    1 \n   6 1371\ntable(predicciones_randomforest)\n\npredicciones_randomforest\n   0    1 \n 197 1180"
  },
  {
    "objectID": "Eda.html#nombre-del-conjunto-de-datos",
    "href": "Eda.html#nombre-del-conjunto-de-datos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "3.1 Nombre del conjunto de datos",
    "text": "3.1 Nombre del conjunto de datos\nKOI(Kepler Objects of Interest) cumulative table."
  },
  {
    "objectID": "Eda.html#propósito-de-los-datos",
    "href": "Eda.html#propósito-de-los-datos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "3.2 Propósito de los datos",
    "text": "3.2 Propósito de los datos\nLa tabla acumulativa de KOI recopila información de las tablas individuales de actividad KOI que describen los resultados actuales de diferentes búsquedas de las curvas de luz de Kepler. El propósito de la tabla acumulativa es proporcionar en un solo lugar las disposiciones más precisas, así como información estelar y planetaria para todos los KOI. Toda la información en esta tabla tiene su origen en otras tablas de actividad KOI."
  },
  {
    "objectID": "Eda.html#origen",
    "href": "Eda.html#origen",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "3.3 Origen",
    "text": "3.3 Origen\n\n3.3.1 a) Origen de los datos\nLos datos han sido recogidos por el observatorio espacial Kepler de la NASA a lo largo de casi 10 años. Para más información acerca del satélite, ver Wikipedia - Telescopio Espacial Kepler.\n\n\n3.3.2 b) Origen del conjunto de datos\nNASA Exoplanet Archive. NASA Exoplanet Science Institute. https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=cumulative.\n\n\n3.3.3 c) Fecha de descarga\nDiciembre de 2023."
  },
  {
    "objectID": "Eda.html#usos-anteriores-del-conjunto-de-datos.",
    "href": "Eda.html#usos-anteriores-del-conjunto-de-datos.",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "3.4 Usos anteriores del conjunto de datos.",
    "text": "3.4 Usos anteriores del conjunto de datos.\n\n3.4.1 a) Uso original:\nDetectar planetas fuera de nuestro sistema solar, es decir, exoplanetas.\n\n\n3.4.2 b) Otros usos:\nDado que el conjunto de datos es público, ha sido ampliamente utilizado en numerosos trabajos. A continuación, se citan algunos ejemplos:\n\nEl trabajo realizado por Aditeya Baral (@aditeyabaral), Ameya Bhamare (@ameyabhamare), y Saarthak Agarwal (@saarthak-agarwal) se encuentra disponible en el repositorio de GitHub: kepler-exoplanet-analysis.\nBatalha, N. M. (2014). “Exploring exoplanet populations with NASA’s Kepler Mission.” Proceedings of the National Academy of Sciences (PNAS)."
  },
  {
    "objectID": "Eda.html#número-de-instancias.",
    "href": "Eda.html#número-de-instancias.",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "3.5 Número de instancias.",
    "text": "3.5 Número de instancias.\nUn total de 9564 observaciones."
  },
  {
    "objectID": "Eda.html#información-de-los-atributos.",
    "href": "Eda.html#información-de-los-atributos.",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "3.6 Información de los atributos.",
    "text": "3.6 Información de los atributos.\nEn el conjunto de datos aparecen varios tipos de IDs: kepid es el identificador asociado a una observación específica, kepler_name es el nombre de dicha estrella y kepoi_name es el nombre asociado a un candidato a exoplaneta. También aparece koi_disposition que es la disposición del archivo de exoplanetas que nos dirá como clasifican las observaciones entre CANDIDATE, FALSE POSITIVE, CONFIRMED y en el caso de la disposición de Kepler koi_pdisposition como CANDIDATE o FALSE POSITIVE. Además también aparecen otras 22 variables para determinar la disposición de la observación.\n\n\n\n\n\n\n\n\n\nNombre\nTipo de dato\nUnidad de medida\nDescripción\n\n\n\n\nkepid\nIdentificador único\n-\nIdentificación única del objeto de interés\n\n\nkepoi_name\nIdentificador único\n-\nIdentificación única del objeto de interés dado por Kepler.\n\n\nkepler_name\nNominal\n-\nNombre del planeta según la nomenclatura de Kepler.\n\n\nkoi_disposition\nNominal\n-\nDisposición del Archivo de Exoplanetas.\n\n\nkoi_pdisposition\nNominal\n-\nDisposición utilizando Datos de Kepler.\n\n\nkoi_score\nContinuo\n-\nPuntuación de Disposición.\n\n\nkoi_fpflag_nt\nBinario\n-\nBandera de Falso Positivo No Similar a Tránsito.\n\n\nkoi_fpflag_ss\nBinario\n-\nBandera de Falso Positivo Eclipse Estelar.\n\n\nkoi_fpflag_co\nBinario\n-\nBandera de Falso Positivo Desplazamiento del Centroide.\n\n\nkoi_fpflag_ec\nBinario\n-\nBandera de Falso Positivo Contaminación Indicada por Coincidencia Efeméride.\n\n\nkoi_period\nContinuo\nDías\nPeríodo Orbital en días.\n\n\nkoi_period_err1\nContinuo\nDías\nIncertidumbre Superior del Período Orbital en días.\n\n\nkoi_period_err2\nContinuo\nDías\nIncertidumbre Inferior del Período Orbital en días.\n\n\nkoi_time0bk\nContinuo\nBKJD (Barycentric Kepler Julian Date)\nÉpoca de Tránsito en BKJD (Barycentric Kepler Julian Date).\n\n\nkoi_time0bk_err1\nContinuo\nBKJD (Barycentric Kepler Julian Date)\nIncertidumbre Superior de la Época de Tránsito en BKJD.\n\n\nkoi_time0bk_err2\nContinuo\nBKJD (Barycentric Kepler Julian Date)\nIncertidumbre Inferior de la Época de Tránsito en BKJD.\n\n\nkoi_impact\nContinuo\n-\nParámetro de Impacto.\n\n\nkoi_impact_err1\nContinuo\n-\nIncertidumbre Superior del Parámetro de Impacto.\n\n\nkoi_impact_err2\nContinuo\n-\nIncertidumbre Inferior del Parámetro de Impacto.\n\n\nkoi_duration\nContinuo\nHoras\nDuración del Tránsito en horas.\n\n\nkoi_duration_err1\nContinuo\nHoras\nIncertidumbre Superior de la Duración del Tránsito en horas.\n\n\nkoi_duration_err2\nContinuo\nHoras\nIncertidumbre Inferior de la Duración del Tránsito en horas.\n\n\nkoi_depth\nContinuo\nppm (partes por millón)\nProfundidad del Tránsito en partes por millón (ppm).\n\n\nkoi_depth_err1\nContinuo\nppm (partes por millón)\nIncertidumbre Superior de la Profundidad del Tránsito en ppm.\n\n\nkoi_depth_err2\nContinuo\nppm (partes por millón)\nIncertidumbre Inferior de la Profundidad del Tránsito en ppm.\n\n\nkoi_prad\nContinuo\nRadios Terrestres\nRadio Planetario en radios terrestres.\n\n\nkoi_prad_err1\nContinuo\nRadios Terrestres\nIncertidumbre Superior del Radio Planetario en radios terrestres.\n\n\nkoi_prad_err2\nContinuo\nRadios Terrestres\nIncertidumbre Inferior del Radio Planetario en radios terrestres.\n\n\nkoi_teq\nContinuo\nKelvin\nTemperatura de Equilibrio en Kelvin.\n\n\nkoi_teq_err1\nContinuo\nKelvin\nIncertidumbre Superior de la Temperatura de Equilibrio en Kelvin.\n\n\nkoi_teq_err2\nContinuo\nKelvin\nIncertidumbre Inferior de la Temperatura de Equilibrio en Kelvin.\n\n\nkoi_insol\nContinuo\nFlujo Terrestre\nFlujo de Insolación en flujo terrestre.\n\n\nkoi_insol_err1\nContinuo\nFlujo Terrestre\nIncertidumbre Superior del Flujo de Insolación en flujo terrestre.\n\n\nkoi_insol_err2\nContinuo\nFlujo Terrestre\nIncertidumbre Inferior del Flujo de Insolación en flujo terrestre.\n\n\nkoi_model_snr\nContinuo\n-\nRelación señal-ruido del tránsito.\n\n\nkoi_tce_plnt_num\nEntero\n-\nNúmero de Planeta TCE.\n\n\nkoi_tce_delivname\nNominal\n-\nEntrega TCE.\n\n\nkoi_steff\nContinuo\nKelvin\nTemperatura Efectiva Estelar en Kelvin.\n\n\nkoi_steff_err1\nContinuo\nKelvin\nIncertidumbre Superior de la Temperatura Efectiva Estelar en Kelvin.\n\n\nkoi_steff_err2\nContinuo\nKelvin\nIncertidumbre Inferior de la Temperatura Efectiva Estelar en Kelvin.\n\n\nkoi_slogg\nContinuo\n\\(log_{10}(cm/s^2)\\)\nGravedad Superficial Estelar en \\(log_{10}(cm/s^2)\\) .\n\n\nkoi_slogg_err1\nContinuo\n\\(log_{10}(cm/s^2)\\)\nIncertidumbre Superior de la Gravedad Superficial Estelar en \\(log_{10}(cm/s^2)\\) .\n\n\nkoi_slogg_err2\nContinuo\n\\(log_{10}(cm/s^2)\\)\nIncertidumbre Inferior de la Gravedad Superficial Estelar en \\(log_{10}(cm/s^2)\\) .\n\n\nkoi_srad\nContinuo\nRadios Solares\nRadio Estelar en radios solares.\n\n\nkoi_srad_err1\nContinuo\nRadios Solares\nIncertidumbre Superior del Radio Estelar en radios solares.\n\n\nkoi_srad_err2\nContinuo\nRadios Solares\nIncertidumbre Inferior del Radio Estelar en radios solares.\n\n\nra\nContinuo\nGrados Decimales\nAscensión Recta en grados decimales.\n\n\ndec\nContinuo\nGrados Decimales\nDeclinación en grados decimales.\n\n\nkoi_kepmag\nContinuo\n-\nMagnitud en la banda Kepler.\n\n\n\nCantidad de atributos por tipo:\nEl conjunto de datos incluye un total de 49 atributos. Entre ellos, tenemos 2 identificadores únicos, hay 4 atributos categóricos, los cuales son nominales. Además, se presentan 16 atributos numéricos, distribuidos en 9 atributos continuos y 7 atributos discretos. Por último, hay que mencionar que aparecen 22 atributos de errores que indican el error superior e inferior de 11 variables."
  },
  {
    "objectID": "Eda.html#atributos-nominales",
    "href": "Eda.html#atributos-nominales",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "6.1 Atributos nominales",
    "text": "6.1 Atributos nominales\nLos 4 atributos nominales son kepler_name, koi_disposition, koi_pdisposition y koi_tce_delivname. A continuación daremos la frecuencia de dichas variables.\nEl atributo kepler_name es el nombre del astro una vez confirmada su existencia. Vemos que hemos detectado con éxito 6821 astros y no necesariamente son exoplanetas, estudiando las demás variables veremos cuántos lo son en verdad.\n\ndatos$kepler_name[datos$kepler_name == \"\"] &lt;- \"Sin datos\"\nsum(datos$kepler_name == \"Sin datos\") \n\n[1] 6821\n\n\nLa siguiente variable es koi koi_disposition que nos dice finalmente la disposición de la observación. Observamos que hay cerca de 2000 observaciones que están pendientes, más de 2700 exoplanetas confirmados y cerca de 5000 falsos positivos. Una posible respuesta al alto número de falsos positivos puede ser que las observaciones se traten de astoreoides o cometas en vez de exoplanetas.\n\ntable(datos$koi_disposition)\n\n\n     CANDIDATE      CONFIRMED FALSE POSITIVE \n          1984           2741           4839 \n\n\nA continuación vemos la frecuencia de la variable koi_pdisposition que nos indica como clasifica el telescopio Kepler las observaciones, como vemos el número de flasos positivos es similar al de la realidad. Más a delante estudiaremos esta clasificación.\n\ntable(datos$koi_pdisposition)\n\n\n     CANDIDATE FALSE POSITIVE \n          4717           4847 \n\n\nPor último estudiaremos la frecuencia de koi_tce_delivname, este atributo nos indica cuando se recibieron los datos.\n\nunique(datos$koi_tce_delivname)\n\n[1] \"q1_q17_dr25_tce\" \"q1_q16_tce\"      \"\"                \"q1_q17_dr24_tce\"\n\ndatos$koi_tce_delivname[datos$koi_tce_delivname == \"\"] &lt;- \"Sin datos\"\n\nlength(unique(datos$koi_tce_delivname))\n\n[1] 4\n\ntable(datos$koi_tce_delivname)\n\n\n     q1_q16_tce q1_q17_dr24_tce q1_q17_dr25_tce       Sin datos \n            796             368            8054             346"
  },
  {
    "objectID": "Eda.html#atributos-continuos",
    "href": "Eda.html#atributos-continuos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "6.2 Atributos continuos",
    "text": "6.2 Atributos continuos\nLo primero que hacemos es eliminar las columnas que no contienen datos continuos y realizaremos un análisis de dichos atributos\n\ndatoscontinuos&lt;- datos[,-c(1,2,3,4,5,7,8,9,10,20,21)]\nresultado_skim &lt;- skim(datoscontinuos)\nresultado_skim &lt;- resultado_skim[,-c(1,3,4,12)]\ntabla_interactiva &lt;- rhandsontable(resultado_skim, selectCallback = TRUE)\ntabla_interactiva\n\n\n\n\n\n\nAhora mostraremos los histogramas de las variables.\n\nsuppressMessages({\n         suppressWarnings({\n                     for (col in colnames(datoscontinuos)) {\n                                     p &lt;- ggplot(datos, aes(x = datoscontinuos[[col]])) +\n                                      geom_histogram(fill = rojo, color = \"black\", alpha = 0.7) +\n                                      ggtitle(paste(\"Histograma de\", col)) +\n                                      labs(x=col) +\n                                      theme_minimal() +\n                                      theme(panel.grid = element_blank(),\n                                      panel.background = element_rect(colour = \"black\", size = 2),\n                                      plot.title = element_text(hjust = 0.5))\n                          \n  print(p)  \n}\n})})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAhora podemos agrupar las variables según su distribución.\nKoi_period, koi_impact, koi_depth, koi_prad, koi_insol, koi_srad están distribuidas a la izquierda concentrándose en el 0.\nKoi_score sigue una distribución bimodal muy pronunciada en sus extremos.\nKoi_time0bk ,koi_teq, koi_duration y koi_model_snr siguen una exponencial.\nKoi_kepmag, ra y koi_slogg son betas.\nKoi_steff sigue una normal.\ndes parece una campana porque decae en los extremos aunque no tiene pico en el centro."
  },
  {
    "objectID": "Eda.html#atributos-discretos",
    "href": "Eda.html#atributos-discretos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "6.3 Atributos discretos",
    "text": "6.3 Atributos discretos\n\ndatosdiscretos&lt;- datos[,c(7,8,9,10,20)]\nfrecuenciakoi_fpflag_nt&lt;- table(datosdiscretos$koi_fpflag_nt)\nbarplot(frecuenciakoi_fpflag_nt, col = rojo,\n        main = \"Gráfico de Barras de Frecuencias\",\n        xlab = \"Bandera de Falso Positivo No Similar a Tránsito. \",\n        ylab = \"Frecuencia\")\n\n\n\n\n\nfrecuenciakoi_fpflag_ss&lt;- table(datosdiscretos$koi_fpflag_ss)\nbarplot(frecuenciakoi_fpflag_ss, col = rojo,\n        main = \"Gráfico de Barras de Frecuencias\",\n        xlab = \"Bandera de Falso Positivo Eclipse Estelar. \",\n        ylab = \"Frecuencia\")\n\n\n\n\n\nfrecuenciakoi_fpflag_co&lt;- table(datosdiscretos$koi_fpflag_co)\nbarplot(frecuenciakoi_fpflag_co, col = rojo,\n        main = \"Gráfico de Barras de Frecuencias\",\n        xlab = \"Bandera de Falso Positivo Desplazamiento del Centroide.\",\n        ylab = \"Frecuencia\")\n\n\n\n\n\nfrecuenciakoi_fpflag_ec&lt;- table(datosdiscretos$koi_fpflag_ec)\nbarplot(frecuenciakoi_fpflag_ec, col = rojo,\n        main = \"Gráfico de Barras de Frecuencias\",\n        xlab = \"Bandera de Falso Positivo Contaminación Indicada por Coincidencia Efeméride. \",\n        ylab = \"Frecuencia\")\n\n\n\n\nEstas banderas nos indican cuando ha ocurrido un falso positivo.\n\nfrecuenciakoi_tce_plnt_num&lt;- table(datosdiscretos$koi_tce_plnt_num)\nbarplot(frecuenciakoi_tce_plnt_num, col = rojo,\n        main = \"Gráfico de Barras de Frecuencias\",\n        xlab = \"Número de Planeta TCE.\",\n        ylab = \"Frecuencia\")"
  },
  {
    "objectID": "clusters.html",
    "href": "clusters.html",
    "title": "3  Diversos tipos de clusterización",
    "section": "",
    "text": "4 Segmentación de clientes"
  },
  {
    "objectID": "clusters.html#análisis-previo",
    "href": "clusters.html#análisis-previo",
    "title": "3  Diversos tipos de clusterización",
    "section": "4.1 Análisis previo",
    "text": "4.1 Análisis previo\nComenzaremos cargando las librerías que vamos a utilizar:\n\nsuppressMessages({\n    suppressWarnings({\n          library(skimr)\n          library(corrplot)\n          library(Rtsne)\n          library(cluster)\n          library(kernlab)\n          library(skmeans)\n          library(ggplot2)\n          library(dplyr)\n          library(DescTools)\n          library(Rtsne)\n          library(irlba)\n          library(Matrix)\n          library(leaflet)\n          library(tidyverse)  \n          library(cluster)    \n          library(factoextra) \n          library(data.table)\n          library(rgl)\n          library(magick)\n          library(rglwidget)\n          library(meanShiftR)\n          library(kernlab)\n\n})})\n\nAjustamos una semilla para realizar correctamente el clustering\n\nset.seed(1234)\n\nAhora cargamos nuestro conjunto de datos y los mostramos.\n\ndatos &lt;- read.csv(file=\"C:/Users/Miguel/Desktop/Matemáticas/Aprendizaje Automático/Credit_Card_Segmentation.csv\", header=T)\nhead(datos)\n\n  CUST_ID    BALANCE BALANCE_FREQUENCY PURCHASES ONEOFF_PURCHASES\n1  C10001   40.90075          0.818182     95.40             0.00\n2  C10002 3202.46742          0.909091      0.00             0.00\n3  C10003 2495.14886          1.000000    773.17           773.17\n4  C10004 1666.67054          0.636364   1499.00          1499.00\n5  C10005  817.71434          1.000000     16.00            16.00\n6  C10006 1809.82875          1.000000   1333.28             0.00\n  INSTALLMENTS_PURCHASES CASH_ADVANCE PURCHASES_FREQUENCY\n1                  95.40        0.000            0.166667\n2                   0.00     6442.945            0.000000\n3                   0.00        0.000            1.000000\n4                   0.00      205.788            0.083333\n5                   0.00        0.000            0.083333\n6                1333.28        0.000            0.666667\n  ONEOFF_PURCHASES_FREQUENCY PURCHASES_INSTALLMENTS_FREQUENCY\n1                   0.000000                         0.083333\n2                   0.000000                         0.000000\n3                   1.000000                         0.000000\n4                   0.083333                         0.000000\n5                   0.083333                         0.000000\n6                   0.000000                         0.583333\n  CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX PURCHASES_TRX CREDIT_LIMIT  PAYMENTS\n1               0.000000                0             2         1000  201.8021\n2               0.250000                4             0         7000 4103.0326\n3               0.000000                0            12         7500  622.0667\n4               0.083333                1             1         7500    0.0000\n5               0.000000                0             1         1200  678.3348\n6               0.000000                0             8         1800 1400.0578\n  MINIMUM_PAYMENTS PRC_FULL_PAYMENT TENURE\n1         139.5098         0.000000     12\n2        1072.3402         0.222222     12\n3         627.2848         0.000000     12\n4               NA         0.000000     12\n5         244.7912         0.000000     12\n6        2407.2460         0.000000     12\n\n\nVamos a explicar cada una de las variables:\nCUST_ID: Identificación del titular de la tarjeta de crédito (Categórico).\nBALANCE: Cantidad de saldo restante en su cuenta para realizar compras.\nBALANCE_FREQUENCY: Con qué frecuencia se actualiza el saldo, puntuación entre 0 y 1 (1 = actualizado con frecuencia, 0 = no actualizado con frecuencia).\nPURCHASES: Cantidad de compras realizadas desde la cuenta.\nONEOFF_PURCHASES: Cantidad máxima de compra realizado de una sola vez.\nINSTALLMENTS_PURCHASES: Cantidad de compra realizado en cuotas.\nCASH_ADVANCE: Efectivo adelantado proporcionado por el usuario.\nPURCHASES_FREQUENCY: Con qué frecuencia se realizan las compras, puntuación entre 0 y 1 (1 = compras frecuentes, 0 = compras no frecuentes).\nONEOFFPURCHASESFREQUENCY: Con qué frecuencia se realizan compras de una sola vez (1 = compras frecuentes, 0 = compras no frecuentes).\nPURCHASESINSTALLMENTSFREQUENCY: Con qué frecuencia se realizan compras a plazos (1 = se realizan con frecuencia, 0 = no se realizan con frecuencia).\nCASHADVANCEFREQUENCY: Con qué frecuencia se realiza el pago de efectivo por adelantado.\nCASHADVANCETRX: Número de transacciones realizadas con “Efectivo por adelantado”.\nPURCHASES_TRX: Número de transacciones de compra realizadas.\nCREDIT_LIMIT: Límite de la tarjeta de crédito para el usuario.\nPAYMENTS: Cantidad de pago realizado por el usuario.\nMINIMUM_PAYMENTS: Cantidad de pagos realizados por el usuario.\nPRCFULLPAYMENT: Porcentaje de pago completo realizado por el usuario.\nTENURE: Duración del servicio de la tarjeta de crédito para el usuario.\nVamos a estudiar cuales de las variables anteriores tienen valores ausentes.\n\nvalores_faltantes &lt;- colSums(is.na(datos))\nprint(valores_faltantes)\n\n                         CUST_ID                          BALANCE \n                               0                                0 \n               BALANCE_FREQUENCY                        PURCHASES \n                               0                                0 \n                ONEOFF_PURCHASES           INSTALLMENTS_PURCHASES \n                               0                                0 \n                    CASH_ADVANCE              PURCHASES_FREQUENCY \n                               0                                0 \n      ONEOFF_PURCHASES_FREQUENCY PURCHASES_INSTALLMENTS_FREQUENCY \n                               0                                0 \n          CASH_ADVANCE_FREQUENCY                 CASH_ADVANCE_TRX \n                               0                                0 \n                   PURCHASES_TRX                     CREDIT_LIMIT \n                               0                                1 \n                        PAYMENTS                 MINIMUM_PAYMENTS \n                               0                              313 \n                PRC_FULL_PAYMENT                           TENURE \n                               0                                0 \n\n\nPara lidiar con ellos los sustituiremos por la moda de cada variable. Primero nos quedaremos con los datos sin la ID del cliente y mostraremos un resumen de sus estadísticas.\n\ndatossinID &lt;- datos[,2:18]\nskim(datos)\n\n\nData summary\n\n\nName\ndatos\n\n\nNumber of rows\n8950\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nCUST_ID\n0\n1\n6\n6\n0\n8950\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nBALANCE\n0\n1.00\n1564.47\n2081.53\n0.00\n128.28\n873.39\n2054.14\n19043.14\n▇▁▁▁▁\n\n\nBALANCE_FREQUENCY\n0\n1.00\n0.88\n0.24\n0.00\n0.89\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nPURCHASES\n0\n1.00\n1003.20\n2136.63\n0.00\n39.63\n361.28\n1110.13\n49039.57\n▇▁▁▁▁\n\n\nONEOFF_PURCHASES\n0\n1.00\n592.44\n1659.89\n0.00\n0.00\n38.00\n577.41\n40761.25\n▇▁▁▁▁\n\n\nINSTALLMENTS_PURCHASES\n0\n1.00\n411.07\n904.34\n0.00\n0.00\n89.00\n468.64\n22500.00\n▇▁▁▁▁\n\n\nCASH_ADVANCE\n0\n1.00\n978.87\n2097.16\n0.00\n0.00\n0.00\n1113.82\n47137.21\n▇▁▁▁▁\n\n\nPURCHASES_FREQUENCY\n0\n1.00\n0.49\n0.40\n0.00\n0.08\n0.50\n0.92\n1.00\n▇▂▂▂▇\n\n\nONEOFF_PURCHASES_FREQUENCY\n0\n1.00\n0.20\n0.30\n0.00\n0.00\n0.08\n0.30\n1.00\n▇▁▁▁▁\n\n\nPURCHASES_INSTALLMENTS_FREQUENCY\n0\n1.00\n0.36\n0.40\n0.00\n0.00\n0.17\n0.75\n1.00\n▇▁▂▁▃\n\n\nCASH_ADVANCE_FREQUENCY\n0\n1.00\n0.14\n0.20\n0.00\n0.00\n0.00\n0.22\n1.50\n▇▁▁▁▁\n\n\nCASH_ADVANCE_TRX\n0\n1.00\n3.25\n6.82\n0.00\n0.00\n0.00\n4.00\n123.00\n▇▁▁▁▁\n\n\nPURCHASES_TRX\n0\n1.00\n14.71\n24.86\n0.00\n1.00\n7.00\n17.00\n358.00\n▇▁▁▁▁\n\n\nCREDIT_LIMIT\n1\n1.00\n4494.45\n3638.82\n50.00\n1600.00\n3000.00\n6500.00\n30000.00\n▇▂▁▁▁\n\n\nPAYMENTS\n0\n1.00\n1733.14\n2895.06\n0.00\n383.28\n856.90\n1901.13\n50721.48\n▇▁▁▁▁\n\n\nMINIMUM_PAYMENTS\n313\n0.97\n864.21\n2372.45\n0.02\n169.12\n312.34\n825.49\n76406.21\n▇▁▁▁▁\n\n\nPRC_FULL_PAYMENT\n0\n1.00\n0.15\n0.29\n0.00\n0.00\n0.00\n0.14\n1.00\n▇▁▁▁▁\n\n\nTENURE\n0\n1.00\n11.52\n1.34\n6.00\n12.00\n12.00\n12.00\n12.00\n▁▁▁▁▇\n\n\n\n\n\nUna vez visto el resumen podemos sustituir los valores ausentes\n\nfor (i in 1:8950){\n  if (is.na(datossinID$CREDIT_LIMIT[i])==TRUE)\n    datossinID$CREDIT_LIMIT[i]&lt;-3000\n  \n  if (is.na(datossinID$MINIMUM_PAYMENTS[i])==TRUE)\n    datossinID$MINIMUM_PAYMENTS[i]&lt;-312\n}\n\nAhora comprobamos si los hemos eliminado bien.\n\nvalores_faltantessinid &lt;- colSums(is.na(datossinID))\nprint(valores_faltantessinid)\n\n                         BALANCE                BALANCE_FREQUENCY \n                               0                                0 \n                       PURCHASES                 ONEOFF_PURCHASES \n                               0                                0 \n          INSTALLMENTS_PURCHASES                     CASH_ADVANCE \n                               0                                0 \n             PURCHASES_FREQUENCY       ONEOFF_PURCHASES_FREQUENCY \n                               0                                0 \nPURCHASES_INSTALLMENTS_FREQUENCY           CASH_ADVANCE_FREQUENCY \n                               0                                0 \n                CASH_ADVANCE_TRX                    PURCHASES_TRX \n                               0                                0 \n                    CREDIT_LIMIT                         PAYMENTS \n                               0                                0 \n                MINIMUM_PAYMENTS                 PRC_FULL_PAYMENT \n                               0                                0 \n                          TENURE \n                               0 \n\n\nA continuación representaremos la correlación entre las variables.\n\ncorrelation_matrix &lt;- cor(datossinID)\ncorrplot(correlation_matrix, tl.cex = 0.6)\n\n\n\n\nComo vemos no hay grandes correlaciones salve entre oneoff_purchases y purchases; purchases_installments_frequency y purchases_frequency y entre cash_advance_trx y cash_advance_frequency.\nA continuación realizaremos un análisis de componentes principales para ver si podemos reducir el número de variables con el objetivo de representar de una manera más comprensiva los reesultados del clustering.\n\npr.out &lt;- prcomp(datossinID, scale = TRUE)\nbiplot(pr.out, scale = 0)\n\n\n\n\nEn esta primera imagen no observamos una gran segmentación ni de las variables ni de los clientes. Esto se debe a que las dos primeras componentes recogen poca información, menos del 50%. Las siguientes líneas nos muestran la varianza de cada una de las componentes principales\n\npr.var &lt;- pr.out$sdev^2\npr.var\n\n [1] 4.640605e+00 3.453344e+00 1.498243e+00 1.271519e+00 1.058200e+00\n [6] 9.758084e-01 8.301820e-01 7.308692e-01 6.457006e-01 5.236004e-01\n[11] 4.032827e-01 3.014728e-01 2.427358e-01 2.068763e-01 1.721534e-01\n[16] 4.539592e-02 1.164886e-05\n\n\nAhora viendo el porcentaje total de cada componente.\n\npve &lt;- pr.var / sum(pr.var) \npve*100\n\n [1] 2.729768e+01 2.031379e+01 8.813195e+00 7.479524e+00 6.224706e+00\n [6] 5.740049e+00 4.883424e+00 4.299230e+00 3.798239e+00 3.080002e+00\n[11] 2.372251e+00 1.773370e+00 1.427858e+00 1.216919e+00 1.012667e+00\n[16] 2.670348e-01 6.852268e-05\n\n\nLas dos primeras tan solo recogen un 47 % de la información por lo que se descarta un análisis en dos dimensiones. Los dos siguientes gráficos recogen esta información:\n\n plot(pve, xlab = \"Número de Componentes Principales\",\n     ylab = \"Proporción de varianza explicada\", ylim = c(0, 1),\n     type = \"b\")\n\n\n\n\n\nplot(cumsum(pve), xlab = \"Número de Componentes Principales\",\n     ylab = \"Suma acumulada de la proporción de varianza explicada\",\n     ylim = c(0, 1), type = \"b\")\n abline(h = 0.9, col = \"red\", lty = 2)\n\n\n\n\nPara quedarnos con el 90% de la información necesitamos 9 componentes principales. Dado que con PCA no conseguimos reducir la información lo suficiente necesitamos realizar tsne.\n\nsuppressMessages({\n         suppressWarnings({\n                     for (col in colnames(datossinID)) {\n                                     p &lt;- ggplot(datos, aes(x = datossinID[[col]])) +\n                                      geom_histogram(fill = \"blue\", color = \"black\", alpha = 0.7) +\n                                      ggtitle(paste(\"Histograma de\", col)) +\n                                      labs(x=col) +\n                                      theme_minimal() +\n                                      theme(panel.grid = element_blank(),\n                                      panel.background = element_rect(colour = \"black\", size = 2),\n                                      plot.title = element_text(hjust = 0.5))\n                          \n  print(p)  \n}\n})})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA la vista de los histogramas dado que casi todas las variables tienen outliers y por la distribución de las mismas hemos considerado realizar una transformación de escala logarítmica.\n\ndatossinID &lt;- datossinID + 0.01\nlogscaled_data &lt;- log(datossinID)\nhead(logscaled_data)\n\n   BALANCE BALANCE_FREQUENCY PURCHASES ONEOFF_PURCHASES INSTALLMENTS_PURCHASES\n1 3.711393      -0.188522342  4.558183        -4.605170               4.558183\n2 8.071680      -0.084370141 -4.605170        -4.605170              -4.605170\n3 7.822108       0.009950331  6.650512         6.650512              -4.605170\n4 7.418589      -0.436392466  7.312560         7.312560              -4.605170\n5 6.706525       0.009950331  2.773214         2.773214              -4.605170\n6 7.500993       0.009950331  7.195405        -4.605170               7.195405\n  CASH_ADVANCE PURCHASES_FREQUENCY ONEOFF_PURCHASES_FREQUENCY\n1    -4.605170        -1.733488674               -4.605170186\n2     8.770743        -4.605170186               -4.605170186\n3    -4.605170         0.009950331                0.009950331\n4     5.326895        -2.371581536               -2.371581536\n5    -4.605170        -2.371581536               -2.371581536\n6    -4.605170        -0.390576003               -4.605170186\n  PURCHASES_INSTALLMENTS_FREQUENCY CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX\n1                       -2.3715815              -4.605170     -4.605170186\n2                       -4.6051702              -1.347074      1.388791241\n3                       -4.6051702              -4.605170     -4.605170186\n4                       -4.6051702              -2.371582      0.009950331\n5                       -4.6051702              -4.605170     -4.605170186\n6                       -0.5219995              -4.605170     -4.605170186\n  PURCHASES_TRX CREDIT_LIMIT  PAYMENTS MINIMUM_PAYMENTS PRC_FULL_PAYMENT\n1   0.698134722     6.907765  5.307337         4.938206        -4.605170\n2  -4.605170186     8.853667  8.319484         6.977608        -1.460061\n3   2.485739636     8.922660  6.433063         6.441417        -4.605170\n4   0.009950331     8.922660 -4.605170         5.743035        -4.605170\n5   0.009950331     7.090085  6.519656         5.500447        -4.605170\n6   2.080690761     7.495547  7.244276         7.786243        -4.605170\n   TENURE\n1 2.48574\n2 2.48574\n3 2.48574\n4 2.48574\n5 2.48574\n6 2.48574\n\n\nEscalamos los datos para que el clustering sea correcto, probaremos dos tipos de datos escalados.\n\ndatosscale &lt;- scale(logscaled_data)\n\nmin_max_scaler &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ndatosminmax &lt;- apply(logscaled_data, 2, min_max_scaler)\n\nAhora analizamos el que resume mejor las variables usando prcomp. En primer lugar usando scale.\n\npr.out.scale &lt;- prcomp(datosscale, scale = FALSE)\npr.var.scale &lt;- pr.out.scale$sdev^2\npve.scale &lt;- pr.var.scale / sum(pr.var.scale) \npve.scale*100\n\n [1] 37.24672762 19.78470977  9.81932040  7.34273399  6.76233041  6.09434665\n [7]  4.47485267  3.11802535  2.54528557  1.51672098  0.54548194  0.30460971\n[13]  0.17438736  0.12803679  0.07075673  0.03967998  0.03199407\n\n\nAhora veamos usando minmax.\n\npr.out.minmax &lt;- prcomp(datosminmax, scale = FALSE)\npr.var.minmax &lt;- pr.out.minmax$sdev^2\npve.minmax &lt;- pr.var.minmax / sum(pr.var.minmax) \npve.minmax*100\n\n [1] 50.07807845 18.12462096 13.85645029  7.72985289  3.07520049  2.20033784\n [7]  1.78875222  0.93513000  0.73499394  0.47214334  0.30009622  0.24045645\n[13]  0.20627630  0.08770111  0.07881230  0.04705275  0.04404443\n\n\nComo vemos obtenemos mejor resultado usando minmax y las 3 primeras componentes representan más del 80% de la información.\n\n(pve.minmax[1]+pve.minmax[2]+pve.minmax[3])*100\n\n[1] 82.05915\n\n\nDibujamos los datos en las 3 primeras componentes principales\n\nlibrary(plot3D)\n\nWarning: package 'plot3D' was built under R version 4.3.2\n\npca_data &lt;- pr.out.minmax$x[, 1:3]\n#plot3d( pca_data, col = \"blue\", type = \"s\", radius = .2 )\n#play3d(spin3d(axis = c(1,0,0), rpm = 4), duration = 20) Si queremos ver el gráfico 3D\n#rglwidget()\nscatter3D(pca_data[, 1], pca_data[, 2], pca_data[, 3], col = \"red\", pch = 16,\n          xlab = \"Componente 1\", ylab = \"Componente 2\", zlab = \"Componente 3\")"
  },
  {
    "objectID": "clusters.html#pam",
    "href": "clusters.html#pam",
    "title": "3  Diversos tipos de clusterización",
    "section": "4.2 PAM",
    "text": "4.2 PAM\nEl primer método que usaremos será PAM. PAM (Partitioning Around Medoids) es un algoritmo de agrupamiento que identifica medoides, puntos representativos, en lugar de centroides como en k-medias. A diferencia de k-medias, PAM es robusto a valores atípicos, ya que utiliza observaciones reales en lugar de promedios. El algoritmo selecciona medoides que minimizan la distancia total a otros puntos dentro de cada clúster. Lo primero que haremos será usar el método para ver el número de clusters que decidimos tomar.\n\nn_clust &lt;- 2:10\npam_list &lt;- lapply(n_clust, function(x) pam(datosminmax, k = x))\n\nPara ello veremos los valores del ancho del coeficiente de silueta (sil_width). Este coeficiente nos proporciona una forma de medir la separación y cohesión de los grupos, el mayor valor será el valor óptimo.\n\nsil_width &lt;- lapply(pam_list, function(x) mean(x$silinfo$widths[, \"sil_width\"]))\n#plot(n_clust, sil_width, type=\"l\")\nsil_df &lt;- data.frame(Clusters = n_clust, Silhouette_Width = unlist(sil_width))\nggplot(sil_df, aes(x = Clusters, y = Silhouette_Width)) +\n  geom_line(size = 1.5) +  \n  geom_point(color = \"red\") +\n  geom_text(aes(label = round(Silhouette_Width, 3)), nudge_y = -0.01, color = \"red\", size = 4) +  \n  labs(title = \"Valores del Ancho del Coeficiente de Silueta\",\n       x = \"Número de clusters\",\n       y = \"Ancho del Coeficiente de Silueta\") +\n  scale_x_continuous(breaks = seq(2, 10, by = 1), labels = abs(seq(2, 10, by = 1))) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.line = element_line(colour = \"black\"),      \n        axis.text.y = element_blank(),     \n        axis.ticks.y = element_blank(),    \n        panel.background = element_rect(colour = \"black\", size = 2),\n        plot.title = element_text(hjust = 0.5))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nComo vemos en el gráfico el valor óptimo se alcanza con 9 clusters muy a la par con el valor con 10 clusters. Ahora representaremos el gráfico con las 3 componentes usando estos 9 clusters.\n\npam_result &lt;- pam_list[[8]]\n\ncluster_labelsPAM &lt;- pam_result$clustering\n\nplot3d(pca_data, col = cluster_labelsPAM + 1, type = \"s\", radius = 0.2)\nrglwidget()\n\n\n\n\n\nOtra forma de representarlos será usando tsne. A continuación cargamos tsne y guardamos los datos usando los clusters del PAM anterior.\n\ntsne_result &lt;- Rtsne(datosminmax)\ntsne_dfPAM &lt;- data.frame(x = tsne_result$Y[, 1], y = tsne_result$Y[, 2], Cluster = factor(cluster_labelsPAM))\n\nA continuación mostramos el gráfico.\n\nggplot(tsne_dfPAM, aes(x, y, color = Cluster)) +\n  geom_point(size = 3) +\n  labs(title = \"Visualización t-SNE con colores PAM para 9 Clusters\", color = \"Cluster\",x=\"\",y=\"\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),      \n        axis.ticks.y = element_blank(),     \n        panel.background = element_rect(colour = \"black\", size=2),\n        plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "clusters.html#agneshclust",
    "href": "clusters.html#agneshclust",
    "title": "3  Diversos tipos de clusterización",
    "section": "4.3 AGNES(hclust)",
    "text": "4.3 AGNES(hclust)\nEl segundo método que usaremos será AGNES. Como vimos en clase este método da mejor resultados usando como distancia ward.D2. Lo primero que haremos será mostrar su dendograma.\n\nclus &lt;- hclust(dist(datosminmax),method=\"ward.D2\")\nplot(clus)\n\n\n\n\nSi elegimos 9 clusters como vimos anteriormente el dendograma y el tsne quedarían como sigue:\n\nplot(clus)\nrect.hclust(clus, k = 9, border = 1:9) \n\n\n\ncluster_labelsHCLUST &lt;- cutree(clus, k = 9)\ntsne_dfAGNES &lt;- data.frame(x = tsne_result$Y[, 1], y = tsne_result$Y[, 2], Cluster = factor(cluster_labelsHCLUST))\n\nggplot(tsne_dfAGNES, aes(x, y, color = Cluster)) +\n  geom_point(size = 3) +\n  labs(title = \"Visualización t-SNE con colores AGNES para 9 Clusters\", color = \"Cluster\",x=\"\",y=\"\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),      \n        axis.ticks.y = element_blank(),     \n        panel.background = element_rect(colour = \"black\", size=2),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\nA simple vista parece que se mezclan menos que cuando usabamos PAM, aunque ambas representaciones son muy similares. Podemos probar a realizarlo cortando el dendograma por 6, en cajas más grandes.\n\nplot(clus)\nrect.hclust(clus, k = 6, border = 1:6) \n\n\n\ncluster_labelsHCLUST &lt;- cutree(clus, k = 6)\ntsne_dfAGNES &lt;- data.frame(x = tsne_result$Y[, 1], y = tsne_result$Y[, 2], Cluster = factor(cluster_labelsHCLUST))\n\nggplot(tsne_dfAGNES, aes(x, y, color = Cluster)) +\n  geom_point(size = 3) +\n  labs(title = \"Visualización t-SNE con colores AGNES para 6 Clusters\", color = \"Cluster\",x=\"\",y=\"\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),      \n        axis.ticks.y = element_blank(),     \n        panel.background = element_rect(colour = \"black\", size=2),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\nComo vemos también obtenemos buenos resultados."
  },
  {
    "objectID": "clusters.html#mean-shift",
    "href": "clusters.html#mean-shift",
    "title": "3  Diversos tipos de clusterización",
    "section": "4.4 Mean Shift",
    "text": "4.4 Mean Shift\nEl tercer método, Mean Shift, implica una técnica de agrupamiento no paramétrica. Se basa en desplazar iterativamente los puntos de datos hacia las regiones de mayor densidad en el espacio, convergiendo hacia los modos locales. Este enfoque encuentra naturalmente el número óptimo de clusters sin la necesidad de especificarlos previamente.\n\ndatosms &lt;- as.matrix(datosminmax)\n\nmsresult &lt;- meanShift(datosms, epsilon = 1e-3)\ncluster_labelsMeanShift &lt;- msresult$assignment\n\ntsne_dfMeanShift &lt;- data.frame(x = tsne_result$Y[, 1], y = tsne_result$Y[, 2], Cluster = factor(cluster_labelsMeanShift))\n\nggplot(tsne_dfMeanShift, aes(x, y, color = Cluster)) +\n  geom_point(size = 3) +\n  labs(title = \"Visualización t-SNE con colores Mean Shift \", color = \"Cluster\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.background = element_rect(colour = \"black\", size = 2),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\nEste método sin embargo, no diferencia tan bien como los anteriores, al menos por lo que vemos a la hora de realziar el tsne. Además toma 15 clusters complicando su representación y su segmentación."
  },
  {
    "objectID": "clusters.html#spectral-clustering",
    "href": "clusters.html#spectral-clustering",
    "title": "3  Diversos tipos de clusterización",
    "section": "4.5 Spectral Clustering",
    "text": "4.5 Spectral Clustering\nPor último, el cuarto método será Spectral Clustering, una técnica que utiliza la estructura de los eigenvectores de la matriz de afinidad para agrupar datos de manera eficiente y capturar patrones no lineales en conjuntos de datos. Esta técnica es especialmente útil para identificar clústeres con formas complejas y no convencionales. Debido a tiempo de cálculo realizaremos spectral con 3 centros y usando tan solo 2000 datos. Para ello haremos un sample de 2000 observaciones para que sea más aleatorio.\n\nposiciones &lt;- sample(1:8950, 2000)\n\nscresult &lt;- specc(as.matrix(datosminmax[posiciones, ]), centers=3)\n\ntsne_result &lt;- Rtsne(datosminmax[posiciones, ])\n\ntsne_dfSpectral &lt;- data.frame(x = tsne_result$Y[, 1], y = tsne_result$Y[, 2], Cluster =factor(scresult))\n\nggplot(tsne_dfSpectral, aes(x, y, color = Cluster)) +\n  geom_point(size = 3) +\n  labs(title = \"Visualización t-SNE con colores Spectral Clustering para 3 Clusters\", color = \"Cluster\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.background = element_rect(colour = \"black\", size = 2),\n        plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "clusters.html#conclusiones",
    "href": "clusters.html#conclusiones",
    "title": "3  Diversos tipos de clusterización",
    "section": "4.6 Conclusiones",
    "text": "4.6 Conclusiones\nPor lo visto anteriormente, si hemos de elegir un número de segmentaciones sería de 9. Además, los modelos que mejores resutlados nos han aportado han sido PAM y AGNES, métodos más sencillos que los dos últimos. A pesar de los resultados de spectral son resultados con menor cantidad de observaciones.\nTal vez sean modelos complejos que necesiten de mayor cantidad de datos para generar resultados decentes. Incluso necesitraíamos mayor potencia pues hemos tenido que reducir el número de observaciones para poder realizar spectral_clusteering"
  },
  {
    "objectID": "Rainprediction.html",
    "href": "Rainprediction.html",
    "title": "4  Predicción de lluvia en Australia",
    "section": "",
    "text": "5 Resumen ejecutivo.\nEn particular, los atributos RainToday y RainTomorrow se comportan de manera binaria (Yes/No) considerando que Yes aparece solo si de 9am a 9pm se registró más de 1 mm de precipitación.\nCantidad de atributos por tipo:\nLa capacidad de predecir el tiempo ha sido, desde tiempos antiguos, una de las tareas más apasionantes y vitales en el ámbito de la ciencia y la tecnología. A lo largo de la historia, la humanidad ha observado los cambios en el cielo y las condiciones atmosféricas para adaptarse a su entorno y tomar decisiones fundamentadas, ya sea en la agricultura, la navegación o la protección ante eventos climáticos extremos. A medida que avanzan los avances científicos y tecnológicos, hemos progresado desde la dependencia de observaciones empíricas hacia la predicción del tiempo basada en datos, modelos matemáticos y observaciones científicas precisas.\nLa predicción del tiempo desempeña un rol crítico en la vida cotidiana. Desde la planificación de actividades al aire libre hasta la toma de decisiones cruciales en el mundo empresarial, la información precisa sobre las condiciones climáticas se convierte en un activo esencial. Además, en un mundo en constante evolución marcado por el calentamiento global y el cambio climático, la predicción del tiempo juega un papel vital en la comprensión y mitigación de los efectos de eventos climáticos extremos.\nEsta introducción destaca la importancia de la predicción del tiempo en nuestra vida diaria y su relevancia a nivel global. En la actualidad, a medida que nos adentramos en la era de la ciencia de datos y la innovación tecnológica, las modernas técnicas de predicción meteorológica se han vuelto más precisas y avanzadas. En este contexto, exploraremos varios modelos para predecir el tiempo usando el aprendizaje automático.\nTras realizar esta observación del conjunto de datos nos hemos dado cuenta de los problemas que tendremos que afrontar en las siguientes fases.\nTendremos que estudiar el problema de los valores ausentes que aparecen en gran proporción en algunas variables. Una posible solución será eliminar del conjunto de datos o rellenar esos valores de alguna forma. También queda pendiente estudiar los máximos y mínimos para confirmar si son o no outliers una vez que tratemos los datos. Los atributos redundantes analizados también serán objeto de transformación o eliminación en la próxima\nA su vez tendremos que profundizar en la climatología de Australia para resolver el inconveniente que generan las estaciones meteorológicas aisladas debido a la gran extensión de la isla. Al incluirlas en un árbol de decisión podríamos estar sobreajustando y tal vez su estudio sea mejor realizarlo de manera independiente.\nPor último, a la hora de evaluar nuestros modelos tendremos que tener cuidado con las clases desbalanceadas. Como objetivos nos podemos plantear superar la predicción que tendríamos si predijesemos que nunca va a llover mañana.\nA la vista de los resultados y las métricas estudiadas en cada uno de los modelos el segundo modelo, correspondiente a la regresión logística, es el que mejores resultados reporta. Huelga añadir que la complejidad del modelo no se relaciona necesariamente con mejores resultados. De hecho, en este caso, pese a poder considerar el random forest como un modelo más avanzado, la regresión logística se comporta de manera más satisfactoria.\nLa decisión de considerar el segundo modelo sobre el primero radica fundamentalmente en que es el que mejor clasifica a la clase Yes. En problemas similares al tratado en este proyecto, en los cuales las clases están extremandamente desbalanceadas, es capital que el modelo sea capaz de clasificar correctamente la clase con menor número de instancias. La complejidad del tratamiento de conjuntos de datos de este tipo no radica en conseguir una tasa de erro baja, pues un clasificador que devuelva la clase No de manera indiscriminada parecería ser idóneo. En cambio, nadad más lejos de la realidad, es necesario utilizar otras métricas que permitan determinar si realmete se está tratando de manera correcta la clase minoritaria.\nAdemás, el segundo modelo no solo clasifica mejor la clase Yes, sino que examinando las métricas una a una se observa que presenta un mayor AUC así como un mayor área bajo la curva PR. Estas consideraciones refuerzan la idea de tomar el modelo de regresión logística como modelo más adecuado en este proyecto.\nEn cualquier caso, este modelo aún es muy mejorable y se puede adecuar en mayor medida al problema tratado. Ya comentamos en anteriores entregables que una de las variables más determinantes en la predicción meteorológica son las relativas a las nubes, tanto su forma como su color como su porcentaje de ocupación en el cielo. Por otro lado tampoco disponemos de datos acerca de variaciones de la presión que son bastante útiles en estos problemas meteorológicos. Dado que nuestro conjunto de datos no posee esta información no podemos tratar de conseguir milagros. Teniendo esto en cuenta, tal y como hemos ido indicando en las entregas previas, hemos valorado multiples soluciones posibles y mejoras del modelo subsanando deficiencias que se han reseñado cuando se han observado. Entre ellas se encuentra por ejemplo la separación del atributo de dirección del viento en seno y coseno, facilitando el tratamiento de estos datos de especial relevancia. Se han valorado también formas de introducir nuevas variables que den cuenta de la variabilidad de la presión o su tendencia a lo largo de los últimos días. Desgraciadamente el tiempo es finito y, pese a que nos habría encantado poder desarrollar con mayor profundidad este proyecto y llevar a cabo todas nuestras ideas en pos de perfeccionar el modelo todo lo posible, hemos tenido que conformarnos con unos resultados para nada desmerecedores pero que distan del tratamiento que habríamos querido dar al problema.\nPor último queremos remarcar la importancia del clustering realizado, que aporta una nueva variable completamente nueva y con una riqueza de información notable. De hecho, únicamente el hecho de haberla incluido ha permitido comparar parámetros que de otra forma habrían sido inalcanzables. Esto es notorio, por ejemplo, en el parámetro maxBins, que se exploró en el entregable individual y se veía acotado por el número de valores del atributo Locations. Reducir estos 49 valores a un total de 4 clusters conlleva una ampliación notable en el rango de valores que se pueden considerar del citado parámetro. Pero lo mejor del procedimiento aplicado es que además de reducir la dimensionalidad se ha añadido información en el proceso, pues los modelos que tienen en cuenta el cluster acceden a información sobre el clima de la estación, cosa que escapa a la capacidad de los modelos que sólo tienen en cuenta Location."
  },
  {
    "objectID": "Rainprediction.html#análisis-general-del-conjunto-de-datos.",
    "href": "Rainprediction.html#análisis-general-del-conjunto-de-datos.",
    "title": "4  Predicción de lluvia en Australia",
    "section": "7.1 Análisis general del conjunto de datos.",
    "text": "7.1 Análisis general del conjunto de datos.\nEl conjunto de datos contiene observaciones de 49 estaciones repartidas por toda Australia. A continuación mostraremos en una imagen sus ubicaciones. Resulta lógico pensar que dependiendo del clima de la región la probabilidad de llover será mayor o menor, por tanto también representaremos los diferentes climas de Australia.\n\n7.1.1 Figuras\nLas siguientes figuras muestran información relevante sobre las estaciones y climas en Australia.\n\n\n\nFigura 1. Ubicación de las estaciones\n\n\n\n\n\nFigura 2. Distintos climas en Australia\n\n\nEn la figura 1 podemos observar como el mayor número de estaciones se encuentra en la zona sureste del mapa. Esto se debe a que es la zona más poblada de Australia, donde se encuentran las grandes ciudades como Sydney o Melbourne. Una pregunta lógica tras ver las ubicaciones de la estaciones es pensar en si será mejor tratar las estaciones aisladas como independientes de las demás y estudiarlas a parte. Debido al gran tamaño de Australia podemos pensar en si realmente influye el tiempo al este con el tiempo al oeste. En la Sección Análisis de las precipitaciones a lo largo del año estudiaremos la lluvia en distintas estaciones para reforzar este pensamiento. A continuación analizaremos la figura 3 que representa el porcentaje de missing values de cada variable.\n\n\n\nFigura 3. Histograma de las diferentes variables numéricas.\n\n\nLas variables con mayor porcentaje de NA son Sunshine, Evaporation, Cloud9am y Cloud3pm. Esto se puede deber a que estas medidas solo se hayan tomado ciertos años o sean difíciles de medir como Evaporation. Las siguientes variables con mayor porcentaje son las relativas a la presión y la humedad pero en menor medida que las anteriores. Podemos pensar que se debe a una falta de herramientas o a problemas técnicos. Las demás variables tienen niveles relativamente bajos de NAs. Algo importante a destacar es que las variables Date y Location no tienen datos faltantes en todo el conjunto, esto nos ahorrará problemas más adelante."
  },
  {
    "objectID": "Rainprediction.html#análisis-de-las-variables-categóricas",
    "href": "Rainprediction.html#análisis-de-las-variables-categóricas",
    "title": "4  Predicción de lluvia en Australia",
    "section": "7.2 Análisis de las variables categóricas",
    "text": "7.2 Análisis de las variables categóricas\nLas tablas que se presentan a continuación representan el recuento total de cada valor de las variables WindGustDir, WindDir9am, WindDir3pm, RainToday y RainTomorrow. Usando las tablas podemos ver la frecuencia de cada uno de los valores y, por ende, saber la que aparece con mayor frecuencia. También aparece el recuento de los NAs de cada variable.\n\n7.2.1 Tabla 1. WindGustDir\n\n\n\nWindGustDir\ncount\npercentage\n\n\n\n\nE\n9181\n6.3117 %\n\n\nENE\n8104\n5.5713 %\n\n\nESE\n7372\n5.0681 %\n\n\nN\n9313\n6.4024 %\n\n\nNA\n10326\n7.0989 %\n\n\nNE\n7133\n4.9038 %\n\n\nNNE\n6548\n4.5016 %\n\n\nNNW\n6620\n4.5511 %\n\n\nNW\n8122\n5.5837 %\n\n\nS\n9168\n6.3028 %\n\n\nSE\n9418\n6.4746 %\n\n\nSSE\n9216\n6.3358 %\n\n\nSSW\n8736\n6.0058 %\n\n\nSW\n8967\n6.1646 %\n\n\nW\n9915\n6.8163 %\n\n\nWNW\n8252\n5.6730 %\n\n\nWSW\n9069\n6.2347 %\n\n\n\n\n\n7.2.2 Tabla 2. WindDir9am\n\n\n\nWindDir9am\ncount\npercentage\n\n\n\n\nE\n9176\n6.3083 %\n\n\nENE\n7836\n5.3870 %\n\n\nESE\n7630\n5.2454 %\n\n\nN\n11758\n8.0833 %\n\n\nNA\n10566\n7.2639 %\n\n\nNE\n7671\n5.2736 %\n\n\nNNE\n8129\n5.5885 %\n\n\nNNW\n7980\n5.4860 %\n\n\nNW\n8749\n6.0147 %\n\n\nS\n8659\n5.9528 %\n\n\nSE\n9287\n6.3846 %\n\n\nSSE\n9112\n6.2643 %\n\n\nSSW\n7587\n5.2159 %\n\n\nSW\n8423\n5.7906 %\n\n\nW\n8459\n5.8153 %\n\n\nWNW\n7414\n5.0969 %\n\n\nWSW\n7024\n4.8288 %\n\n\n\n\n\n7.2.3 Tabla 3. WindDir3pm\n\n\n\nWindDir3pm\ncount\npercentage\n\n\n\n\nE\n8472\n5.8243 %\n\n\nENE\n7857\n5.4015 %\n\n\nESE\n8505\n5.8470 %\n\n\nN\n8890\n6.1116 %\n\n\nNA\n4228\n2.9066 %\n\n\nNE\n8263\n5.6806 %\n\n\nNNE\n6590\n4.5305 %\n\n\nNNW\n7870\n5.4104 %\n\n\nNW\n8610\n5.9192 %\n\n\nS\n9926\n6.8239 %\n\n\nSE\n10838\n7.4508 %\n\n\nSSE\n9399\n6.4616 %\n\n\nSSW\n8156\n5.6070 %\n\n\nSW\n9354\n6.4306 %\n\n\nW\n10110\n6.9504 %\n\n\nWNW\n8874\n6.1006 %\n\n\nWSW\n9518\n6.5434 %\n\n\n\n\n\n7.2.4 Tabla 4. RainToday\n\n\n\nRainToday\ncount\npercentage\n\n\n\n\nNA\n3261\n2.2419 %\n\n\nNo\n110319\n75.8415 %\n\n\nYes\n31880\n21.9167 %\n\n\n\n\n\n7.2.5 Tabla 5. RainTomorrow\n\n\n\nRainTomorrow\ncount\npercentage\n\n\n\n\nNA\n3267\n2.2460 %\n\n\nNo\n110316\n75.8394 %\n\n\nYes\n31877\n21.9146 %\n\n\n\nEn las Tablas 1, 2 y 3 se presentan las frecuencias asociadas a diversas características del viento. La Tabla 1 refleja la dirección del viento más predominante, mientras que la Tabla 2 se enfoca en la dirección del viento a las 9 de la mañana, y la Tabla 3 analiza la dirección del viento a las 3 de la tarde. Es importante tener en cuenta que estas variables también exhiben una dependencia significativa de la estación climática. Por lo tanto, nuestras observaciones se expresan en un contexto general, ya que las direcciones predominantes del viento pueden variar en estaciones climáticas aisladas en comparación con aquellas observadas en diferentes ubicaciones.\nLas Tablas 4 y 5 nos indican lo desbalanceada que está la variable respuesta de este problema de clasificación. Es evidente que podríamos realizar predicciones considerando que no lloverá al día siguiente, logrando una tasa de acierto superior al 75 %. Es decir, acertaríamos en 3 de cada 4 ocasiones. Nuestro principal objetivo radica en lograr una clasificación precisa de las clases, y no tanto en minimizar el error general aparente.\nLa siguiente tabla nos indica los valores distintos de las variables categóricas que no habíamos introducido en la entrega 1. Vemos que tenemos 49 estaciones de lluvia, 3436 fechas distintas, las direcciones de viento están divididas en 16 secciones, la variable que nos indica si ha llovido hoy o no y por último la clase que indica si lloverá mañana o no.\n\n\n7.2.6 Tabla 6. Valores distintos para variables categóricas\n\n\n\nVariable\nValores distintos\n\n\n\n\nLocation\n49\n\n\nDate\n3436\n\n\nWindGustDir\n16\n\n\nWindDir9am\n16\n\n\nWindDir3pm\n16\n\n\nRainToday\n2\n\n\nRainTomorrow\n2\n\n\n\nSi dividimos 3436 entre 365 obtenemos la cantidad de años que tenemos recogidos, casi 9 años y medio. También si multiplicamos 49 por 3436 obtenemos un valor mayor al del total de las instacias del conjunto de datos, esto nos dice que hay fechas en las que alguna estación no recogió valores.\nLas variables con respecto al viento vemos que tienen 16 valores distintos, esto quiere decir que el círculo está dividido en 16 y aparecen valores como NNW(nornoroeste)\n\n\n7.2.7 Correlaciones entre las variables categóricas\nCon el objetivo de estudiar la relación entre las variables categóricas hemos realizado varias pruebas \\(\\chi^2\\) que expondremos a continuación.\nRainToday-RainTomorrow: Hemos enfrentado las variables que nos indican si ha llovido y hemos obtenido un p-valor \\(&lt;\\) 2.2e-16. Lo que sugiere que hay una fuerte asociación entre las variables. Es normal que ocurra esto, en general no llueve de manera aislada.\nDate-RainTomorrow: Hemos enfrentado la fecha y la variable de respuesta y hemos obtenido un p-valor \\(&lt;\\) 2.2e-16. Esta fuerte asociación entre las variables se debe a que habrá días y meses en los que será más o menos frecuente la lluvia.\nRainToday-Location: Hemos realizado la prueba usando las variables RainToday Location y hemos obtenido un p-valor \\(&lt;\\) 2.2e-16. Esta relación se debe a que dependiendo de la estación y el clima en concreto de la misma lloverá con mayor o con menor frecuencia.\nWindGustDir-RainTomorrow: Hemos enfrentado las variables y hemos obtenido un p-valor \\(&lt;\\) 2.2e-16. Es un hecho conocido que el viento influye en la climatología, tanto su precedencia como en este caso su fuerza, por eso obtenemos esta fuerte relación.\nWindDir9am-WindDir3pm: Hemos enfrentado las variables y hemos obtenido un p-valor \\(&lt;\\) 2.2e-16. Evidentemente el test nos indica que la dirección del viento a las 9 de la mañana está fuertemente relacionada con la dirección unas pocas horas después.\nWindDir9am-WindGustDir: Hemos enfrentado las variables y hemos obtenido un p-valor \\(&lt;\\) 2.2e-16. El objetivo de esta prueba era confirmar lo que ya se conoce, la fuerte relación entre el viento más fuerte y el viento a las 9 de la mañana.\nRainTomorrow-WindGustDir: Hemos enfrentado la variable respuesta y la variable “WindGustDir” y hemos obtenido un p-valor \\(&lt;\\) 2.2e-16. Esta prueba nos indica que están fuertemente correladas. Parece lógico pensar que dependiendo de lo fuerte que el viento mueva las nubes lloverá o no."
  },
  {
    "objectID": "Rainprediction.html#análisis-de-las-variables-numéricas",
    "href": "Rainprediction.html#análisis-de-las-variables-numéricas",
    "title": "4  Predicción de lluvia en Australia",
    "section": "7.3 Análisis de las variables numéricas",
    "text": "7.3 Análisis de las variables numéricas\nLa primera tabla contiene el número de valores distintos, la media, la desviación típica, el mínimo y el máximo, valores clave en el análisis de datos.\nLa siguiente nos indica la moda, el porcentaje de NA de cada una de las variables numéricas y el rango de validez de las mismas.\n\n7.3.1 Tabla 7. Estadísticas para diferentes campos\n\n\n\n\n\n\n\n\n\n\n\nCampo\nValores Distintos\nMedia\nDesviación Típica\nMínimo\nMáximo\n\n\n\n\nWindGustSpeed\n67\n40.04\n13.61\n6\n135\n\n\nWindSpeed9am\n43\n14.04\n8.92\n0\n130\n\n\nWindSpeed3pm\n44\n18.66\n8.81\n0\n87\n\n\nHumidity9am\n101\n68.88\n19.03\n0\n100\n\n\nHumidity3pm\n101\n51.54\n20.80\n0\n100\n\n\nCloud9am\n10\n4.45\n2.89\n0\n9\n\n\nCloud3pm\n10\n4.51\n2.72\n0\n9\n\n\nMinTemp\n389\n12.19\n6.40\n-8.5\n33.9\n\n\nMaxTemp\n505\n23.22\n7.12\n-4.8\n48.1\n\n\nRainfall\n681\n2.36\n8.48\n0.0\n371.0\n\n\nEvaporation\n358\n5.47\n4.19\n0.0\n145.0\n\n\nSunshine\n145\n7.61\n3.79\n0.0\n14.5\n\n\nPressure9am\n546\n1017.65\n7.11\n980.5\n1041.0\n\n\nPressure3pm\n549\n1015.26\n7.04\n977.1\n1039.6\n\n\nTemp9am\n441\n16.99\n6.49\n-7.2\n40.2\n\n\nTemp3pm\n502\n21.68\n6.94\n-5.4\n46.7\n\n\n\n\n\n7.3.2 Tabla 8. Moda, Porcentaje de NA y Rango de Validez\n\n\n\nCampo\nModa\nPorcentaje de NA\nRango de Validez\n\n\n\n\nMinTemp\n13\n1.02 %\n[-89.7, 56.7]\n\n\nMaxTemp\n19.6\n0.87 %\n[-89.7, 56.7]\n\n\nRainfall\n0\n2.24 %\n[0, 1500]\n\n\nSunshine\n0\n48.01 %\n[0, 20]\n\n\nWindSpeed9am\n13\n1.21 %\n[0, 372]\n\n\nWindSpeed3pm\n17\n2.10 %\n[0, 372]\n\n\nHumidity9am\n68\n1.82 %\n[0, 100]\n\n\nHumidity3pm\n52\n3.10 %\n[0, 100]\n\n\nPressure9am\n1016.4\n10.36 %\n[920, 1080]\n\n\nPressure3pm\n1013.5 1014.8\n10.33 %\n[920, 1080]\n\n\nCloud9am\n7\n7.65 %\n{0, 1, …, 8}\n\n\nCloud3pm\n7\n7.80 %\n{0, 1, …, 8}\n\n\nTemp9am\n16.6\n1.21 %\n[-89.7, 56.7]\n\n\nTemp3pm\n18.4\n2.48 %\n[-89.7, 56.7]\n\n\n\nEn el análisis de los datos, hemos validado el rango de validez para variables como Humidity y Cloud según sus definiciones. En el caso de variables como MinTemp o Pressure9am, hemos considerado como rango de validez los mínimos y máximos globales registrados. Gracias al código de Scala, hemos verificado que ninguna variable está fuera de su rango de validez.\nAdemás, al observar la media y la desviación estándar de algunas variables, podemos considerar que los valores máximos podrían ser outliers. Por ejemplo, en las variables Rainfall y Evaporation, esto podría deberse a eventos extremos como tormentas. Asimismo, la alta variación de temperaturas se explica por la extensión de Australia y la ubicación de las distintas estaciones en diferentes climas.\n\n\n\nFigura 4. Histograma de las diferentes variables numéricas.\n\n\nAl estudiar los histogramas, podemos agrupar las variables según la distribución a la que se aproximan.\nDistribución normal\nLas variables que siguen una distribución normal, debido a su forma de campana, son las relativas a la temperatura y a la presión: MinTemp, MaxTemp, Temp9am, Temp3pm, Pressure9am y Pressure3pm. También cabe mencionar que la variable Humidity3pm tiene una forma de campana, pero las colas no llegan a 0.\nDistribución Weibull\nLas variables relativas al viento siguen una distribución Weibull debido a su forma de campana con desplazamiento a la izquierda y cola hacia la derecha: WindGustSpeed, WindSpeed9am y WindSpeed3pm.\nDistribución Valle\nLas variables relativas a las nubes tienen una forma de valle, lo que indica que es probable que el cielo esté con pocas nubes o mayormente cubierto, pero menos probable que la mitad del cielo esté cubierto. Estas variables son Cloud9am y Cloud3pm.\nDistribución concentrada en el 0\nLas variables relativas a la lluvia en un día, Rainfall y Evaporation, están concentradas cerca del 0, ya que, como mencionamos antes, llueve aproximadamente 1 de cada 4 días.\nDistribución sesgada a la derecha\nLas variables Humidity9am y Sunshine están sesgadas a la derecha. La humedad tiene sus valores más altos a primera hora de la mañana, si no tenemos en cuenta si ha llovido o no. Por otro lado, las horas de sol están sesgadas hacia la derecha debido a que tienen una duración estándar.\n\n\n7.3.3 Correlaciones entre las variables numéricas\nA continuación vemos una matriz de correlaciones de las variables numéricas que estamos utilizando (figura 5). Nos fijaremos sobre todo en los valores extremos.\n\n\n\nFigura 5. Histograma de las diferentes variables numéricas.\n\n\nPodemos ver que las variables relacionadas con la temperatura están muy correladas entre sí y, en particular, la temperatura máxima está relacionada con la humedad y la presión. Así como las variables que nos aportan datos del viento. Por otro lado, la variable Sunshine está muy correlada con la cantidad de nubes Cloud9am y Cloud3pm y también con la humedad.\nCabe estudiar la existencia de atributos redundantes a partir de esta tabla. Si dos atributos tienen una correlación similar con el resto de variables, ambos aportarán una información similar. Para llevar a cabo esta comparación basta con observar si las columnas y las filas asociadas al par de atributos de interés son similares.\nTal y como cabría esperar, los atributos que cuantifican la misma magnitud en diferentes momentos del día presentan correlaciones extremadamente similares con el resto de atributos. En particular, Humidity9am y Humidity3pm, aun siendo los dos atributos que menos similitud presentan de los anteriormente mencionados, son grandes candidatos a ser modificados en la etapa de procesamiento del dato. Asimismo, los tres atributos relativos a la velocidad del viento presentan una muy clara redundancia.\nPor otro lado, los atributos MinTemp y MaxTemp son bastante similares, pero presentan diferencias notables en sus correlaciones con parte de los demás atributos. Es por ello que hemos considerado que el estudio de su redundancia puede ser más esclarecedor una vez se introduzca el modelo. Esta misma apreciación es también aplicable a la relación de los dos atributos anteriores con Temp9am y Temp3pm."
  },
  {
    "objectID": "Rainprediction.html#análisis-de-las-precipitaciones-a-lo-largo-del-año",
    "href": "Rainprediction.html#análisis-de-las-precipitaciones-a-lo-largo-del-año",
    "title": "4  Predicción de lluvia en Australia",
    "section": "7.4 Análisis de las precipitaciones a lo largo del año",
    "text": "7.4 Análisis de las precipitaciones a lo largo del año\nSe presenta el histograma de la cantidad de registros del conjunto de datos para cada fecha (figura 6).\n\n\n\nFigura 6. Histograma de la cantidad de registros del conjunto de datos para cada fecha.\n\n\nHemos optado por realizar un análisis anual de las precipitaciones para determinar, por ejemplo, las probabilidades de precipitación según el mes del año. Aunque se podría afinar este análisis diariamente, se expondrán a continuación los desafíos que presenta esta aproximación en esta etapa del proyecto.\nEl conjunto de datos abarca el período de 2007 a 2017, específicamente, desde el 1 de noviembre de 2007 hasta el 25 de junio de 2017. Esta franja temporal corresponde a un total de 3.525 días. Sin embargo, al verificar la cantidad de días recogidos en el conjunto de datos, el resultado es de 3.436 días. Es decir, hay 89 días sin información, lo que representa el 2,5% del total.\n\n7.4.1 Cantidad de Registros por Estación Meteorológica\nLa siguiente tabla muestra la cantidad de registros por estación meteorológica desde 2009 hasta 2017.\n\n7.4.1.1 Tabla 9. Registros por estación\n\n\n\n\n\n\n\n\n\n\n\nLocation\ncount\nLocation\ncount\nLocation\ncount\n\n\n\n\nAdelaide\n3009\nMelbourne\n3009\nSydney\n3009\n\n\nAlbany\n3009\nMelbourneAirport\n3009\nSydneyAirport\n3009\n\n\nAlbury\n3009\nMildura\n3009\nTownsville\n3009\n\n\nAliceSprings\n3009\nMoree\n3009\nTuggeranong\n3008\n\n\nBadgerysCreek\n3009\nMountGambier\n3009\nUluru\n1578\n\n\nBallarat\n3009\nMountGinini\n3009\nWaggaWagga\n3009\n\n\nBendigo\n3009\nNewcastle\n3008\nWalpole\n3006\n\n\nBrisbane\n3009\nNhil\n1578\nWatsonia\n3009\n\n\nCairns\n3009\nNorahHead\n3004\nWilliamtown\n3009\n\n\nCanberra\n3009\nNorfolkIsland\n3009\nWitchcliffe\n3009\n\n\nCobar\n3009\nNuriootpa\n3009\nWollongong\n3009\n\n\nCoffsHarbour\n3009\nPearceRAAF\n3009\nWoomera\n3009\n\n\nDartmoor\n3009\nPenrith\n3008\n\n\n\n\nDarwin\n3009\nPerth\n3009\n\n\n\n\nGoldCoast\n3009\nPerthAirport\n3009\n\n\n\n\nHobart\n3009\nPortland\n3009\n\n\n\n\nKatherine\n1578\nRichmond\n3009\n\n\n\n\nLaunceston\n3009\nSale\n3009\n\n\n\n\n\nEn la etapa de tratamiento de los datos, asumimos la periodicidad anual y cierta semejanza entre días y meses cercanos. Aunque los 89 días sin información no están distribuidos homogéneamente en casi 10 años, su ausencia no debería ser relevante tras un correcto tratamiento de los datos.\n\n\n\n7.4.2 Análisis de Ausencias y Distribución Temporal\nEn la figura 6, el histograma representa el número de estaciones diferentes donde se ha realizado una medición en la fecha indicada. Se observa una falta significativa de datos en los años anteriores a 2009 y notables ausencias de datos continuos en 2011, 2012 y 2013.\nEn el subconjunto de años 2009 a 2017, el número total de días considerados es de 3.097, y se tiene registro de 3.009 de ellos. Aproximadamente el 2,8% de los días no tienen registro de ninguna estación meteorológica. Aunque en este subconjunto se agrupan casi todos los registros faltantes, hay mediciones de al menos 45 estaciones meteorológicas todos los días.\nEsto lleva a la conclusión de que sería conveniente considerar solo los datos entre 2009 y 2017 para un análisis anual de las precipitaciones. Se plantea también el tratamiento de los registros ausentes como missing values, al menos en dicho análisis.\n\n\n7.4.3 Diferencia de Precipitaciones en Distintas Ubicaciones\nEn la figura 7, se visualiza la diferencia de precipitaciones a lo largo del 2016 en tres ubicaciones distintas: Darwin al norte, Melbourne al sureste y Perth al oeste.\n\n\n\nFigura 7. Precipitaciones a lo largo de 2016.\n\n\nFinalmente, la próxima etapa del proyecto abordará el tratamiento de los datos y proporcionará una gráfica de las precipitaciones medias por mes, así como la determinación de las posibilidades de precipitación a priori."
  },
  {
    "objectID": "Rainprediction.html#consideraciones-previas",
    "href": "Rainprediction.html#consideraciones-previas",
    "title": "4  Predicción de lluvia en Australia",
    "section": "9.1 Consideraciones previas",
    "text": "9.1 Consideraciones previas\nSiguiendo la metodología CRISP-DM, previamente a la transformación de los datos es necesario comprender en profundidad el problema a tratar. Es por ello que hemos realizado una busqueda bibliográfica con el fin de poder tratar los datos de manera adecuada.\nEn la Sección 3.4 de Cullen se dan indicaciones sobre la parametrización de un modelo meteorológico generalista. Dichas indicaciones permiten hacerse una idea de los atributos más influyentes en las variaciones climáticas y, en particular, en las precipitaciones.\nEn primer lugar, la localización es un atributo extremadamente relevante, tanto es así que incluso convendría tener en cuenta las condiciones meteorológicas en estaciones vecinas para poder hacer predicciones precisas. Esto no es posible con un árbol de decisión si no se realizan profundas transformaciones en el dataset, pero sí es posible introducir manualmente una variable con la zona climática, descomponer la ubicación en longitud y latitud o hacer un clustering para crear un nuevo atributo. En esta etapa hemos intentado hacer un clustering, ya que no aumenta la dimensionalidad tanto como la opción de las coordenadas y permite crear grupos más parecidos que la zona climática, pues pese a que dos estaciones tengan un clima semejante, si están separadas 1.000 km el tiempo no va a estar extremadamente correlado día a día. Desgraciadamente no hemos sido capaces de llevarlo a cabo para esta entrega, pero seguramente lo hagamos de cara a la siguiente ya que dispondremos de más tiempo. De hecho, en posteriores modelos es posible que se opte por probar con una aplicación conjunta del clustering y las consideraciones anteriores.\nEl citado artículo también da cuenta de la relevancia de la temperatura y la humedad en el proceso de formación de nubes. Por el contrario, la cantidad de nubes no parece ser un parámetro especialmente relevante, sino que su composición, el tipo de nube o la evolución de la misma son mucho más determinantes. Desgraciadamente nuestro conjunto no dispone de tales datos, como mucho se puede intuir que los días en los que el cielo está completamente cubierto hay mayor probabilidad de que la nube sea un estrato o un cumulonimbo. En cualquier caso, a tenor de la literatura consultada, no consideramos que sea un atributo que aporte información sustancial sobre las precipitaciones del día siguiente, menos aún teniendo en cuenta la cantidad de datos faltantes que presenta.\nEn el proceso de formación de nubes de lluvia intervienen con especial relevancia la evaporación y el viento, tal y como se describe en Gregory. En dicha publicación se ahonda en un modelo descriptivo de los procesos de convección responsables de la evolución nubosa. En cuanto al viento cabe decir que es extremadamente importante su relación con la localización, pues los vientos provinientes del mar suelen arrastrar humedad y condiciones favorables para la aparición de precipitaciones. De hecho, en el citado artículo se indica incluso la importancia de las condiciones del mar tales como la temperatura en la superficie o la temperatura del propio agua. Si bien no disponemos de tanta información en nuestro conjunto de datos, sí cabe considerar la importancia de la dirección y velocidad del viento así como los diferentes valores de temperatura.\nTal y como se vió en la etapa anterior, la evaporación está negativamente correlada con la humedad, de hecho sus correlaciones con el resto de variables son opuestas (figura 5). Físicamente está influida por la temperatura y, en menor medida, por la presión pues ambas condicionan la presión de vapor, que determina el punto de evaporación. Esto permite justificar la eliminación de la evaporación en el conjunto de datos desde un punto de vista puramente meteorológico.\nPor último, la presión, muy relevante en la previsión meteorológica no tiene por qué ser tan relevante en nuestro caso particular, pues en los modelos meteorológicos como el descrito en Hudson tienen en cuenta su variación y no su valor en un instante concreto. La citada publicación describe, precisamente, el modelo meteorológico utilizado actualmente por la agencia meteorológica de Australia. Tras una consulta del mismo podemos preveer que nuestro modelo no tenga una muy elevada capacidad predictiva ya que la inmensa mayoría de datos considerados son relaciones entre las mediciones de distintas estaciones meteorológicas o la evolución de dichas medidas. Para intentar adecuarse a estas consideraciones, en futuras etapas de este proyecto se valorará la posibilidad de incluir nuevos atributos que contengan la evolución de la presión en los últimos días o la diferencia de temperatura de una estación con respecto a la media de las de su mismo cluster, por ejemplo.\nComprendemos que la naturaleza del método a implementar (árboles de decisión) y las características del dataset pueden provocar que los factores determinantes para la predicción sean diferentes a las tratadas en este epígrafe. En cualquier caso, siguiendo la metodología expuesta en clase, hemos considerado necesario documentarnos e incluir esta información previamente al tratamiento de datos."
  },
  {
    "objectID": "Rainprediction.html#conjuntos-de-entrenamiento-y-prueba",
    "href": "Rainprediction.html#conjuntos-de-entrenamiento-y-prueba",
    "title": "4  Predicción de lluvia en Australia",
    "section": "9.2 Conjuntos de entrenamiento y prueba",
    "text": "9.2 Conjuntos de entrenamiento y prueba\nHemos dividido el dataset de manera aleatoria en dos partes. El conjunto de entrenamiento constituye el 70% de los registros del conjunto de datos mientras que el conjunto de prueba representa el 30%.\nAsí pues, de ahora en adelante, cada vez que se haga referencia en esta memoria a una modificación del conjunto de datos se entiende que se realiza paralelamente en el conjunto de entrenamiento y en el conjunto de prueba."
  },
  {
    "objectID": "Rainprediction.html#limpieza-de-datos",
    "href": "Rainprediction.html#limpieza-de-datos",
    "title": "4  Predicción de lluvia en Australia",
    "section": "9.3 Limpieza de datos",
    "text": "9.3 Limpieza de datos\nTal y como se vió en la etapa previa, nuestro conjunto de datos no contiene registros vacíos o inválidos, por tanto, la limpieza se fundamenta en el análisis de los valores de cada atributo.\nEn la figura 3 se observa cómo la mitad de valores del atributo Sunshine son Na y, en menor medida, Evaporation, Cloud3pm y Cloud9am presentan un número altísimo de missing values. La proporción de valores ausentes es tan elevada que hemos considerado oportuno eliminar las variables pues, como hemos visto, tampoco tienen especial relevancia desde el punto de vista meteorológico.\nTal y como se verá en la próxima sección, las variables categóricas consideradas son WindGustDir, RainToday y RainTomorrow. En cuanto a la primera, sus valores están uniformemente distribuidos, es por ello que no considerábamos oportuno sustituir los valores faltantes por uno de ellos en particular. Dado que la cantidad de Na es baja hemos decidido eliminar los registros.\nEn total se han eliminado 18.620 instancias en el proceso de limpieza. Es decir \\(\\mathrm{NTAIE}=18.620\\), que en términos relativos se corresponde con el valor 0,128."
  },
  {
    "objectID": "Rainprediction.html#selección-atributos",
    "href": "Rainprediction.html#selección-atributos",
    "title": "4  Predicción de lluvia en Australia",
    "section": "9.4 Selección atributos",
    "text": "9.4 Selección atributos\nEl uso de un árbol de decisión aumenta la relevancia de este apartado del diseño del modelo. Los árboles de decisión suelen conducir a mejores resultados cuando la dimensionalidad del conjunto de datos es reducida, sobre todo teniendo en cuenta que el árbol tiene profundidad 5. Consecuentemente cobra sentido la documentación realizada sobre meteorología, permitiendo una reducción manual de la dimensionalidad fundamentada, referenciando a estudios científicos sobre el tema.\nComo ya se ha indicado, no hemos considerado las variables Sunshine, Evaporation, Cloud9am y Cloud3pm por su gran cantidad de valores ausentes y su poca relevancia desde el punto de vista meteorológico.\n\n\n\nFigura 8. Histograma de las diferentes variables numéricas.\n\n\nEn la figura 8 se observa cómo hay una cantidad notable de atributos redundantes, pues no solo están notablemente correlados con algún otro, sino que las correlaciones de ambos con el resto de atributos son similares (u opuestas en el caso de correlación negativa). Este es el motivo de no considerar las variables MaxTemp, WindSpeed9am, WindSpeed3pm, Humidity9am y Pressure9am.\nSe ha preferido conservar las variables con mediciones más próximas al día siguiente, es decir, aquellas terminadas en 3pm. En cuanto a la velocidad del viento, se ha conservado la velocidad máxima por ser la más general.\nEs cierto que todas los atributos de temperatura presentan una elevada correlación entre ellos pero cada uno tiene distintas correlaciones con el resto de variables. Por otro lado, en cuanto a meteorología es lógico conservar tres variables de temperatura pues en todos los documentos referenciados se hace hincapié en la relevancia de esta magnitud sobre el clima. El que sí se ha eliminado es MaxTemp, ya que la inmensa mayoría de los días la temperatura máxima se da a las 3pm y así lo muestra la Figura 8, en términos de correlaciones.\nArgumentos análogos a los anteriores, considerando la alta correlación entre las variables de dirección del viento dada por el test \\(\\chi^2\\) de la entrega anterior justifican la eliminación de WindDir9am y WindDir3pm.\nLa variable Rainfall no se ha eliminado dado que tiene una muy baja correlación con el resto de variables numéricas. Si bien es cierto que está fuertemente correlada con el atributo RainToday aporta información que difícilmente se puede extraer del resto de mediciones pues no es lo mismo una lluvia ligera a que precipitaciones torrenciales. Cabe destacar que los modelos meteorológicos consultados no hacen referencia a la cantidad de precipitaciones pero esto se debe a que son mucho más precisos y se basan en el análisis dinámico de la atmósfera. Es decir, tienen en cuenta la evolución de la misma tanto en el tiempo como en el espacio, lo cual escapa al modelo que nosotros planteamos. Precisamente Rainfall compensa en ciertos casos esa falta de información dinámica pues la cantidad de precipitaciones está ligada al tipo de nube que las provoca, dato al cuál no se tiene acceso de ninguna otra manera. Sencillamente, si llueve mucho un día, será más probable que llueva también al siguiente que si llueve poco.\nAnalizando ahora las variables categóricas, tanto RainToday como WindGustDir no han sido eliminadas por estar fuertemente correladas con RainTomorrow y, en consecuencia, tener gran capacidad predictiva.\nLas variables Day y Month también se han utilizado en el modelo, deshechando la información que aporta el año por considerarla poco relevante desde nuestro punto de vista. Consideramos que la fecha aporta una gran cantidad de información pero por el caracter estacional de las precipitaciones; el año, a lo sumo, puede aportar información sobre la tendencia de las precipitaciones en el largo plazo, pero no creemos que se pueda extraer tal tendencia a partir de una muestra de 10 años.\n\n\n\nFigura 9. Matriz de correlaciones\n\n\nEn conclusión, los atributos seleccionados son los siguientes: Location, Day, Month, MinTemp, Temp9am, Temp3pm, WindGustDir, WindGustSpeed, Humidity3pm, Pressure3pm, Rainfall y RainToday"
  },
  {
    "objectID": "Rainprediction.html#transformación-de-datos",
    "href": "Rainprediction.html#transformación-de-datos",
    "title": "4  Predicción de lluvia en Australia",
    "section": "9.5 Transformación de datos",
    "text": "9.5 Transformación de datos\nPara comenzar este epígrafe queremos discutir el tratamiento de los outliers. La decisión tomada es no transformarlos ni eliminarlos. Esto se debe a las enormes variaciones que se dan en el clima australiano, que pueden provocar eventos extremos que den lugar a outliers. El problema es que muchos eventos extremos como El Niño o La Niña son cíclicos (ver Hudson) y el modelo debe ser capaz de predecirlos. De hecho, es tal la cantidad de ciclos meteorológicos que se dan tanto en el Pacífico como en el Índico que la mayoría de estaciones meteorológicas se ven afectadas por su periodicidad, haciendo imposible determinar qué datos deben ser tratados como outliers y cuáles no, al menos con el nivel de profundización en el tema asumible en este proyecto.\nTal y como se ha indicado anteriormente, hemos separado la variable Date en tres atributos referentes al día, el mes y el año para posteriormente utilizar los dos primeros.\nLos missing values de las variables RainToday y RainTomorrow se han sustituido directamente por No. Esta decisión la hemos tomado basándonos en que la inmensa mayoría de valores son negativos y en que la cantidad de valores faltantes es pequeña.\nSomos conscientes de las repercusiones que puede tener una transformación semejante en conjuntos de datos con clases muy desbalanceadas. Es por ello que esta decisión se ha tomado tras una reflexión sobre el origen de los Na. Consultando la fuente del conjunto de datos se puede constatar que las variables que nos atañen se han obtenido a partir del atributo Rainfall de manera automática. Por tanto, la inexistencia de valores para RainToday y RainTomorrow es consecuencia de la inexistencia de los de Rainfall y cabe preguntarse por qué no se tiene una medida de las precipitaciones esos días.\nAustralia es un país en el que se produce una cantidad notable de eventos climáticos extremos y, por ello, la agencia meteorológica está preparada para ello e intenta recavar la mayor cantidad de información posible sobre ellos. Es por ello que cabe pensar que las medidas de los pluviómetros son más valiosas los días que llueve y por ello intentarán tomarlas a toda costa, por ejemplo, dejando su mantenimiento para temporadas con probabilidad casi nula de lluvia. Consecuentemente, es razonable pensar que una medida Na se puede sustituir por un No asumiendo que las medidas correspondientes a los valores faltantes tienen una proporción ínfima de Yes, notablemente menor que el resto de registros.\nPor último, los missing values de las variables numéricas han sido sustituidas por la moda dado que, tal y como se comprobó en la anterior etapa, sus distribuciones son centradas y apuntadas por lo general. En cualquier caso, para las posteriores entregas se valorará una posible sustitución más adecuada en cada atributo particular.\nNos hemos planteado también la conversión de las direcciones del viento a dos atributos: el seno y el coseno del ángulo. Este tipo de transformaciones, así como el clustering o la sustitución de los valores numéricos faltantes requieren de una inversión de tiempo que posterga su inclusión en el proyecto aunque nos hubiera gustado comprobar su eficacia en esta etapa. En cualquier caso, el uso de un modelo más simple en esta etapa permite tener un buen baremo de comparación en"
  },
  {
    "objectID": "Rainprediction.html#evaluación-del-modelo",
    "href": "Rainprediction.html#evaluación-del-modelo",
    "title": "4  Predicción de lluvia en Australia",
    "section": "9.6 Evaluación del modelo",
    "text": "9.6 Evaluación del modelo\nTras entrenar el modelo con los datos de entrenamiento y evaluar el rendimiento con los datos de prueba obtenemos los siguientes resultados:\n\n\n\nMétrica\nValor\n\n\n\n\nTasa de Acierto\n0.830\n\n\nError\n0.170\n\n\n\nComo vimos anteriormente el valor relativo de NTAIE era 0,128 que es menor al error obtenido. # Conclusiones entrega 2 El modelo desarrollado tiene una tasa de acierto de 0,830 mientras que en el conjunto de prueba encontramos una distribución de clases de 0,775 No y 0,225 Yes obviando los missing values. Esto significa que el modelo mejora en más de 5 puntos porcentuales la tasa de acierto con respecto a una predicción basada en decir que no va a llover mañana independientemente de las mediciones.\nEn conjuntos con clases tan desbalanceadas es complicado hacer grandes mejoras en la tasa de error, por lo que el resultado obtenido es razonable. Como cabría esperar, dista mucho de los modelos meteorológicos utilizados en la práctica, que alcanzan tasas de acierto superiores a 0,950 pero en ningún caso se espera alcanzar una tasa próxima siquiera. Esto se debe, entre otras cosas, a que no disponemos de datos suficientes para describir los fenómenos climatológicos que provocan precipitaciones, condicionados por un gran número de factores dinámicos y sus variaciones en tiempo y espacio.\nAdemás, la distribución de clases particular de este dataset hace que la tasa de error no sea un evaluador especialmente descriptivo, con lo que es razonable esperar a futuras entregas para evaluar en profundidad del clasificador que hemos desarrollado.\nEn esta etapa del proceso se ha conseguido desarrollar un modelo simple con capacidad predictiva para el problema dado que puede servir en etapas futuras como vara de medir. Así pues, en futuras entregas se intentará mejorar el modelo. Para ello hemos planteado el uso de clustering, de la información sobre las zonas climáticas australianas, la proximidad entre estaciones y las variaciones de presión, entre otros. Queremos destacar el caso del clustering con el objetivo de agrupar las variables que tengan un clima similar, que ha quedado a medias por falta de tiempo y se incluirá casi con total seguridad. Otras transformaciones pendientes, que probablemente se incluyan, son la transformación del día y el mes en una variable que dé cuenta de en qué momento del año nos encontramos, transformar las direcciones del viento en seno y coseno y aplicar algún método de sustitución como una regresión para reducir el número de instancias eliminadas."
  },
  {
    "objectID": "Rainprediction.html#clusterización",
    "href": "Rainprediction.html#clusterización",
    "title": "4  Predicción de lluvia en Australia",
    "section": "10.1 Clusterización",
    "text": "10.1 Clusterización\nPrevio a introducir el grueso del contenido de esta entrega hemos considerado necesario explicar la motivación y análisis del clustering que se ha comentado en las anteriores etapas del proyecto.\nLa idea del clustering surge de considerar que, al igual que el clima de Santander no tiene que ver con el clima de Andalucía, en Australia, debido al incremento de distancias, esta diferencia entre zonas se magnifica. Esto es lo que nos ha motivado a agrupar las distintas estaciones despendiendo de su clima.\nSi bien la introducción manual de la zona climática a la que pertenece cada estación es una opción razonable a la hora de identificar las tendencias que siguen las precipitaciones, hemos considerado prioritario realizar una clusterización de la columna Location. Por un lado, es una consideración con una mayor aplicabilidad dentro de la metodología que se ha aprendido en la asignatura ya que en este caso particular son únicamente 49 localizaciones diferentes, pero para conjuntos de datos más grandes identificar manualmente el clima de cada estación puede pasar de ser una tarea tediosa a ser una tarea potencialmente inviable. Por otro lado, las zonas climáticas de que se dispone se extraen de lo que se conocen como mapas de clasificación de Köppen. Estos mapas son de aplicación global, luego los climas que consideran son generales y pueden variar notablemente dentro de Australia. Es por ello que hemos rechazado la idea de la identificación manual.\nAsí pues, para la realización del clustering se han seleccionado los atributos más relevantes del conjunto de datos para calcular sus promedios y aplicar un algoritmo de k-medias. Dichas variables son:\n\nPromedio temperatura 9am.\nPromedio temperatura 3pm.\nTemperatura mínima.\nPromedio temperatura mínima diaria.\nPromedio cantidad de lluvia diaria.\nPromedio humedad 3pm.\nPromedio presión 3pm.\nSuma días de lluvia.\n\n\n\n\nFigura 10. Zonas climáticas de Australia.\n\n\n\n\n\nFigura 11. Estacionalidad de las lluvias en Australia.\n\n\n\n\n\nFigura 12. Estaciones meteorológicas del dataset.\n\n\nAtendiendo al dibujo ya introducido en el primer entregable (Figura 10) hemos decidido tomar 4 clusters que deberían reducir los tipos de clima así como agrupar climas parecidos. Precisamente el resultado obtenido se muestra en la Figura 12, que permite una comparación rápida de los clusters con las zonas climáticas. Además, se incluye la Figura 11, que a su vez permite cotejar los resultados obtenidos con la estacionalidad de las lluvias.\nLos dos mapas climáticos han sido extraídos directamente de la web del Bureau of Meteorology of Australia Bureau. Por otro lado, en la Figura 12 el color de la chincheta representa el cluster al que pertenece la estación meteorológica.\nLas gráficas siguientes permiten intuir la relación que guardan las estaciones de un mismo cluster. Asimismo, lo razonable de su interpretación sirve de aval a la hora de asumir que el clustering se ha realizado correctamente.\n\n\n\nFigura 13. Representación de las diferentes estaciones tomando sus precipitaciones frente a su temperatura máxima.\n\n\nEn la figura 13 se observa cómo existe una aparente relación entre la temperatura máxima y las precipitaciones de las estaciones que están agrupadas. De hecho el cluster 4 podría corresponderse con zonas con un clima más desértico (pocas precipitaciones y altas temperaturas); el 1 y el 2 presentan precipitaciones notables y altas temperaturas, lo cual podría asimilarse a los climas tropicales y subtropicales, y, por último, el cluster número 3 agrupa lugares con precipitaciones débiles.\n\n\n\nFigura 14. Representación de las diferentes estaciones tomando su temperatura mínima frente a su temperatura máxima.\n\n\nEn la representación de la figura 14 no es tan clara la disgregación de los clusters, pero sí parece separar las estaciones con temperaturas más extremas de las demás. Aparentemente las ubicaciones con temperaturas mínimas más altas se agrupan en dos clusters mientras que aquellas con temperaturas altas mayores lo hacen en los otros dos. Esto no deja de ser curioso pues, al menos para personas con conocimientos muy limitados de meteorología como es nuestro caso, parecería razonable pensar que allí donde se alcanzan mayores temperaturas máximas también se alcanzarán las mínimas. Por el contrario la parte superior derecha del gráfico parece relativamente vacía y los puntos parecen agruparse más bien en torno a una recta decreciente. En cualquier caso la poca correlación que guardan los puntos de la figura 14 no permite extraer conclusiones sobre la relación entre temperaturas máximas y mínimas con un nivel de confianza alto.\n\n\n\nFigura 15. Representación de las diferentes estaciones tomando la humedad media frente a la cantidad de días de lluvia.\n\n\nAl contrario que en el caso anterior, la figura 15 muestra una correlación que sí es esperable entre la humedad y las precipitaciones. Precisamente esta alta correlación permite diferenciar mucho más claramente los clusters, pero consecuentemente, aporta poca información.\n\n\n\nFigura 16. Representación de los en dos dimensiones.\n\n\nTomando ahora variables adecuadas, se obtiene una representación gráfica de los clusters donde se observan claramente sus divisiones y cómo dos de ellos se encuentran notablemente más próximos en el espacio de variables que los otros dos.\nFinalmente, cabe reseñar una última ventaja del clustering con respecto a la consideración de las zonas climáticas y es que pese a que dos estaciones pertenezcan al mismo clima y estén próximas algunas de ellas se encuentran en accidentes geográficos importantes como montañas. Esto implica que pese a su proximidad física y climática, las precipitaciones pueden ser mucho más abundantes en una de ellas. Esto no se puede tener en cuenta considerando únicamente el mapa de la figura 11.\nLos datos de latitud y longitud utilizados en esta sección han sido obtenidos a partir de la información puesta a disposición por el Gobierno de Australia en Locations. En la web citada se pueden buscar una a una las ubicaciones que aparecen en el conjunto de datos y tomar, en la mayoría de casos sin ambigüedad, las coordenadas de la estación próxima que tenga registros entre 2007 y 2017."
  },
  {
    "objectID": "Rainprediction.html#primer-modelo",
    "href": "Rainprediction.html#primer-modelo",
    "title": "4  Predicción de lluvia en Australia",
    "section": "10.2 Primer modelo",
    "text": "10.2 Primer modelo\nEl primer modelo es un Random Forest que hemos entrenado modificando y buscando los parámetros correctos. Hemos modificado en un principio la máxima profundidad, el máximo número de discretizadores de las variables continuas y el número de árboles del bosque. Comenzamos viendo que los mejores valores para estas características son \\(numTrees=20\\), \\(maxDepth=15\\) y \\(maxBins=50\\). Posteriormente realizamos otro modelo viendo si se podía reducir el número de bins ahora que ya no teníamos las 49 estaciones tras haberlas sustituido por su cluster correspondiente.\nPor último probamos las demás características como el menor número de instancias por nodo, el umbral de información necesaria para realizar una división en un nodo, si se deben almacenar los identificadores en caché y el intervalo para almacenar checkpoints. Tras el crossvalidator obtenemos que los parámetros correctos son: número de árboles: 20, profundidad máxima: 15, número máximo de bins: 50, mínimo de instancias por nodo: 5, mínimo de ganancia de información: 0.0, caché de IDs de nodos: false e intervalo de checkpoint: 10.\nEn el conjunto de entrenamiento nos da un AUC 0.8646.\nCon dichos parámetros y en el conjunto de prueba hemos obtenido las siguientes métricas.\n\n\n\nMétrica\nValor\n\n\n\n\nError\n0.153\n\n\nAcierto\n0.847\n\n\nDesviación Estándar\n0.00174\n\n\nIntervalo de Confianza Inferior\n0.149\n\n\nIntervalo de Confianza Superior\n0.156\n\n\nÁrea bajo la curva ROC\n0.717\n\n\nÁrea bajo la curva PR\n0.861\n\n\nTasa de ciertos positivos\n0.955\n\n\nTasa de falsos positivos\n0.520\n\n\n\nResultados métricas Random Forest\nMatriz de confusión del modelo 1 \\[\n\\begin{array}{c|cc}\n    & \\text{0.0} & \\text{1.0} \\\\\n\\hline\n\\text{0.0} & 4632 & 5027 \\\\\n\\text{1.0} & 1497 & 31563 \\\\\n\\end{array}\n\\]\nLa anterior matriz nos muestra como unos los días que llueve al día siguiente y como ceros los que no. Como vemos, nuestro modelo falla en la clase menos prioritaria. Pero acierta en la clase mayaritaria, si no sería un despropósito. A pesar de esto mejoramos el modelo trivial, aquel modelo que decía que nunca llovía.\n\n\n\nFigura 17. Curva Roc Random Forest"
  },
  {
    "objectID": "Rainprediction.html#segundo-modelo",
    "href": "Rainprediction.html#segundo-modelo",
    "title": "4  Predicción de lluvia en Australia",
    "section": "10.3 Segundo modelo",
    "text": "10.3 Segundo modelo\nEl segundo modelo es una regresión logística en la que modificamos los parámetros de iteraciones máximas, la mezcla entre L1 y L2, el threshold es el umbral y la fuerza de regularización. Tras usar validación cruzada los mejores parámetros son \\(regParam= 0.01\\), \\(elasticNetParam= 0.5\\), \\(threshold=0.6\\) y \\(maxIters=10\\) que nos dan un valor de AUC de 0.84798.\nA continuación veremos los valores de las métricas pedidas para la evaluación con el conjunto de prueba.\n\n\n\nMétrica\nValor\n\n\n\n\nError\n0.163\n\n\nAcierto\n0.837\n\n\nDesviación Estándar\n0.00179\n\n\nIntervalo de Confianza Inferior\n0.159\n\n\nIntervalo de Confianza Superior\n0.166\n\n\nÁrea bajo la curva ROC\n0.732\n\n\nÁrea bajo la curva PR\n0.869\n\n\nTasa de ciertos positivos\n0.924\n\n\nTasa de falsos positivos\n0.459\n\n\n\nResultados métricas Regresión Logística\nMás adelante vamos a comparar estos dos resultados.\nMatriz de confusión del modelo 2 \\[\n\\begin{array}{c|cc}\n    & \\text{0.0} & \\text{1.0} \\\\\n\\hline\n\\text{0.0} & 5220 & 4439 \\\\\n\\text{1.0} & 2513 & 30547 \\\\\n\\end{array}\n\\]\nEn esta tabla vemos como se clasifica mejor a los días que va a llover a cambio de perder algunas clasificaciones.\nA continuación vemos la curva ROC de la regresión logística:\n\n\n\nFigura 18. Curva Roc Regresión Logística"
  },
  {
    "objectID": "zumonaranjas.html",
    "href": "zumonaranjas.html",
    "title": "5  Validación cruzada en diversos modelos",
    "section": "",
    "text": "6 ¿En qué tienda se compraron los zumos?"
  },
  {
    "objectID": "zumonaranjas.html#intoducción",
    "href": "zumonaranjas.html#intoducción",
    "title": "5  Validación cruzada en diversos modelos",
    "section": "6.1 Intoducción",
    "text": "6.1 Intoducción\nLos datos contienen 1070 compras en las que el cliente compró Citrus Hill o Minute Maid Orange Juice. Se registran una serie de características del cliente y del producto. El objetivo del conjunto de datos es predecir qué zumo se compró.\nTenemos 6 variables categóricas, una de ellas un factor las demás escritas como números discretos. Por otro lado tenemos 11 variables numéricas.\nCargamos las librerías y vemos los datos, así como su resumen.\n\nsuppressMessages({\n    suppressWarnings({\nlibrary(ISLR2, quietly = TRUE)\nlibrary(corrplot, quietly = TRUE)\nlibrary(ggplot2, quietly = TRUE)\nlibrary(dplyr, quietly = TRUE)\nlibrary(caTools, quietly = TRUE)\nlibrary(caret, quietly = TRUE)\nlibrary(class, quietly = TRUE)\nlibrary(randomForest, quietly = TRUE)\nlibrary(e1071, quietly = TRUE)\nlibrary(MASS, quietly = TRUE)\nlibrary(rpart, quietly = TRUE)\nlibrary(lattice, quietly = TRUE)\nlibrary(nnet, quietly = TRUE)\nlibrary(mlbench, quietly = TRUE)\nlibrary(adabag, quietly = TRUE)\n    })})\nhead(OJ)\n\n  Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH\n1       CH            237       1    1.75    1.99   0.00    0.0         0\n2       CH            239       1    1.75    1.99   0.00    0.3         0\n3       CH            245       1    1.86    2.09   0.17    0.0         0\n4       MM            227       1    1.69    1.69   0.00    0.0         0\n5       CH            228       7    1.69    1.69   0.00    0.0         0\n6       CH            230       7    1.69    1.99   0.00    0.0         0\n  SpecialMM  LoyalCH SalePriceMM SalePriceCH PriceDiff Store7 PctDiscMM\n1         0 0.500000        1.99        1.75      0.24     No  0.000000\n2         1 0.600000        1.69        1.75     -0.06     No  0.150754\n3         0 0.680000        2.09        1.69      0.40     No  0.000000\n4         0 0.400000        1.69        1.69      0.00     No  0.000000\n5         0 0.956535        1.69        1.69      0.00    Yes  0.000000\n6         1 0.965228        1.99        1.69      0.30    Yes  0.000000\n  PctDiscCH ListPriceDiff STORE\n1  0.000000          0.24     1\n2  0.000000          0.24     1\n3  0.091398          0.23     1\n4  0.000000          0.00     1\n5  0.000000          0.00     0\n6  0.000000          0.30     0\n\nsummary(OJ)\n\n Purchase WeekofPurchase     StoreID        PriceCH         PriceMM     \n CH:653   Min.   :227.0   Min.   :1.00   Min.   :1.690   Min.   :1.690  \n MM:417   1st Qu.:240.0   1st Qu.:2.00   1st Qu.:1.790   1st Qu.:1.990  \n          Median :257.0   Median :3.00   Median :1.860   Median :2.090  \n          Mean   :254.4   Mean   :3.96   Mean   :1.867   Mean   :2.085  \n          3rd Qu.:268.0   3rd Qu.:7.00   3rd Qu.:1.990   3rd Qu.:2.180  \n          Max.   :278.0   Max.   :7.00   Max.   :2.090   Max.   :2.290  \n     DiscCH            DiscMM         SpecialCH        SpecialMM     \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.05186   Mean   :0.1234   Mean   :0.1477   Mean   :0.1617  \n 3rd Qu.:0.00000   3rd Qu.:0.2300   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :0.50000   Max.   :0.8000   Max.   :1.0000   Max.   :1.0000  \n    LoyalCH          SalePriceMM     SalePriceCH      PriceDiff       Store7   \n Min.   :0.000011   Min.   :1.190   Min.   :1.390   Min.   :-0.6700   No :714  \n 1st Qu.:0.325257   1st Qu.:1.690   1st Qu.:1.750   1st Qu.: 0.0000   Yes:356  \n Median :0.600000   Median :2.090   Median :1.860   Median : 0.2300            \n Mean   :0.565782   Mean   :1.962   Mean   :1.816   Mean   : 0.1465            \n 3rd Qu.:0.850873   3rd Qu.:2.130   3rd Qu.:1.890   3rd Qu.: 0.3200            \n Max.   :0.999947   Max.   :2.290   Max.   :2.090   Max.   : 0.6400            \n   PctDiscMM        PctDiscCH       ListPriceDiff       STORE      \n Min.   :0.0000   Min.   :0.00000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.140   1st Qu.:0.000  \n Median :0.0000   Median :0.00000   Median :0.240   Median :2.000  \n Mean   :0.0593   Mean   :0.02731   Mean   :0.218   Mean   :1.631  \n 3rd Qu.:0.1127   3rd Qu.:0.00000   3rd Qu.:0.300   3rd Qu.:3.000  \n Max.   :0.4020   Max.   :0.25269   Max.   :0.440   Max.   :4.000  \n\n\nVeamos si hay valores faltantes en los datos.\n\nvalores_faltantes &lt;- colSums(is.na(OJ))\nprint(valores_faltantes)\n\n      Purchase WeekofPurchase        StoreID        PriceCH        PriceMM \n             0              0              0              0              0 \n        DiscCH         DiscMM      SpecialCH      SpecialMM        LoyalCH \n             0              0              0              0              0 \n   SalePriceMM    SalePriceCH      PriceDiff         Store7      PctDiscMM \n             0              0              0              0              0 \n     PctDiscCH  ListPriceDiff          STORE \n             0              0              0 \n\n\nEstudiemos como es la variable storeID, hay 5 posibles tiendas.\n\nunique(OJ$StoreID)\n\n[1] 1 7 2 3 4\n\n\nEsto nos dice que hay variables que contienen la misma información y tendremos que modificar los datos para mejorar su calidad. Ahora veremos como se distribuyen las variables.\n\npar(mfrow=c(2,7))\nhist(OJ$WeekofPurchase, main = \"\")\nhist(OJ$StoreID, main = \"\")\nhist(OJ$PriceCH, main = \"\")\nhist(OJ$PriceMM, main = \"\")\nhist(OJ$DiscCH, main = \"\")\nhist(OJ$DiscMM, main = \"\")\nhist(OJ$LoyalCH, main = \"\")\nhist(OJ$SalePriceMM, main = \"\")\nhist(OJ$SalePriceCH, main = \"\")\nhist(OJ$PriceDiff, main = \"\")\nhist(OJ$PctDiscMM, main = \"\")\nhist(OJ$PctDiscCH, main = \"\")\nhist(OJ$ListPriceDiff, main = \"\")\n\n\n\n\nFijandonos podemos observar caracteristicas de las variables Vemos como las distribuciones suelen estar concentradas en torno a un valor que destaca más que el resto.\nEliminamos las varibales Store7 y STORE porque esta información ya está contenida en StoreID:\n\nOJ &lt;- OJ[, !(names(OJ) %in% \"Store7\")]\nOJ &lt;- OJ[, !(names(OJ) %in% \"STORE\")]\n\nUsamos one-hot para separar la variable StoreID y luego eliminamos esta variable por las nuevas creadas.\n\none_hot_storeid &lt;- model.matrix(~ factor(StoreID) - 1, data = OJ)\ncolnames(one_hot_storeid) &lt;- paste(\"StoreID\", levels(factor(OJ$StoreID)), sep = \"_\")\nOJ &lt;- cbind(OJ, one_hot_storeid)\nOJ &lt;- OJ[, !(names(OJ) %in% \"StoreID\")]\n\nAhora convertimos la clase en numérica y calculamos el porcentaje de cada clase.\n\nOJ$Purchase &lt;- ifelse(OJ$Purchase == \"CH\", 1, 0)\nporcentajeCH &lt;- sum(OJ$Purchase == 1) / nrow(OJ)\nporcentajeMM &lt;- 1 - porcentajeCH\ncat(\"Procentaje de compras en Citrus Hill:\",porcentajeCH)\n\nProcentaje de compras en Citrus Hill: 0.6102804\n\ncat(\"Procentaje de compras en Minute Maid:\",porcentajeMM)\n\nProcentaje de compras en Minute Maid: 0.3897196\n\n\nEstudiamos las correlaciones entre las variables finales.\n\npar(mfrow = c(1, 1))\ncorrelation_matrix &lt;- cor(OJ)\ncorrplot(correlation_matrix,method = 'square',type = 'upper')\n\n\n\n\nAhora separamos nuestro conjunto de datos en datos de entrenamiento y de prueba.\n\nset.seed(1234)\nsplit &lt;- sample.split(OJ$Purchase, SplitRatio = 0.80)\ntraining_set &lt;- subset(OJ, split == TRUE)\ntest_set &lt;- subset(OJ, split == FALSE)\n\nDespués vamos a crear los conjuntos en los que realizaremos la validación cruzada para mejorar el rendimiento del modelo.\n\nfolds &lt;- createFolds(training_set$Purchase, k = 5)\n\nEntrenamos LDA y mostramos los resultados:\n\nlistamodelos &lt;- list()\nlistaprecisiones &lt;- list()\n\nfor (i in 1:5) {\n  training_fold &lt;- training_set[-folds[[i]], ]\n  test_fold &lt;- training_set[folds[[i]], ]\n  clasificador &lt;- lda(Purchase ~ ., data = training_fold)\n  y_pred &lt;- predict(clasificador, newdata = test_fold)$class\n  mc &lt;- table(test_fold$Purchase, y_pred)\n  precision &lt;- (mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] + mc[1,2] + mc[2,1])\n  listamodelos[[i]] &lt;- clasificador\n  listaprecisiones[[i]] &lt;- precision\n}\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nprecisionLDA &lt;- mean(as.numeric(listaprecisiones))\ncat(\"\\nPrecision media validacion cruzada LDA:\",precisionLDA)\n\n\nPrecision media validacion cruzada LDA: 0.8224194\n\ncat(\"\\nError medio validacion cruzada LDA:\",1-precisionLDA)\n\n\nError medio validacion cruzada LDA: 0.1775806\n\nindice&lt;- which.max(listaprecisiones)\nmejormodelo &lt;- listamodelos[[indice]]\npredicciones &lt;- predict(mejormodelo, newdata = test_set)$class\nmc &lt;- table(test_set$Purchase, predicciones)\nprecision &lt;-(mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\ncat(\"\\nPrecision LDA:\",precision)\n\n\nPrecision LDA: 0.8411215\n\ncat(\"\\nError LDA:\",1-precision)\n\n\nError LDA: 0.1588785\n\n\nEn el caso del discriminante cuadrático (QDA) necesitamos que las variables no estén correlacionadas. Para ello tenemos que eliminar gran parte de ellas. Además mostramos un gráfico de las correlaciones restantes.\n\ntraining_setQDA &lt;-training_set[,-c(3,4,7,9,10,11,12,13,14,15,20)]\ntest_setQDA &lt;- test_set[,-c(3,4,7,9,10,11,12,13,14,15,20)]\n\ncorrelation_matrix &lt;- cor(training_setQDA)\ncorrplot(correlation_matrix,method = 'square',type = 'upper')\n\n\n\nlistamodelos &lt;- list()\nlistaprecisiones &lt;- list()\n\nfor (i in 1:5) {\n  training_fold &lt;- training_setQDA[-folds[[i]],]\n  test_fold &lt;- training_setQDA[folds[[i]],]\n  clasificador &lt;- qda(Purchase ~ ., data = training_fold)\n  y_pred &lt;- predict(clasificador, newdata = test_fold)$class\n  mc &lt;- table(test_fold$Purchase, y_pred)\n  precision &lt;- (mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\n  listamodelos[[i]] &lt;- clasificador\n  listaprecisiones[[i]] &lt;- precision\n}\n\nprecisionQDA &lt;- mean(as.numeric(listaprecisiones))\ncat(\"\\nPrecision media QDA:\",precisionQDA)\n\n\nPrecision media QDA: 0.6845505\n\ncat(\"\\nError medio QDA:\",1-precisionQDA)\n\n\nError medio QDA: 0.3154495\n\nindice&lt;- which.max(listaprecisiones)\nmejormodelo &lt;- listamodelos[[indice]]\npredicciones &lt;- predict(mejormodelo, newdata = test_set)$class\nmc &lt;- table(test_set$Purchase, predicciones)\nprecision &lt;-(mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\ncat(\"\\nPrecision LDA:\",precision)\n\n\nPrecision LDA: 0.7009346\n\ncat(\"\\nError aLDA:\",1-precision)\n\n\nError aLDA: 0.2990654\n\n\nEn el caso de k-vecinos más próximos primer calculamos el k-indicado con la validación cruzada y posteriormente lo probamos con el conjunto de prueba con el k que mejor resultado nos aporta.\n\nfor(i in 1:10){\ncvkNN &lt;- lapply(folds, function(x){\n  training_fold &lt;- training_set[-x, ]\n  test_fold &lt;- training_set[x, ]\n  y_pred &lt;- knn(training_fold[, -1], \n                test_fold[, -1], \n                cl = training_fold[, 1], \n                k = i)\n  mc &lt;- table(test_fold$Purchase, y_pred)\n  precision &lt;- (mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\n  return(precision)\n})\nprecisionkNN &lt;- mean(as.numeric(cvkNN))\ncat(\"\\nPrecision knn con k=\",i,\":\",precisionkNN)\ncat(\"\\nError knn con k=\",i,\":\",1-precisionkNN)\n}\n\n\nPrecision knn con k= 1 : 0.705576\nError knn con k= 1 : 0.294424\nPrecision knn con k= 2 : 0.6868829\nError knn con k= 2 : 0.3131171\nPrecision knn con k= 3 : 0.7312933\nError knn con k= 3 : 0.2687067\nPrecision knn con k= 4 : 0.7090915\nError knn con k= 4 : 0.2909085\nPrecision knn con k= 5 : 0.7382905\nError knn con k= 5 : 0.2617095\nPrecision knn con k= 6 : 0.7347953\nError knn con k= 6 : 0.2652047\nPrecision knn con k= 7 : 0.7242214\nError knn con k= 7 : 0.2757786\nPrecision knn con k= 8 : 0.719543\nError knn con k= 8 : 0.280457\nPrecision knn con k= 9 : 0.7359241\nError knn con k= 9 : 0.2640759\nPrecision knn con k= 10 : 0.7195566\nError knn con k= 10 : 0.2804434\n\nknn_model &lt;- knn(training_set[, -1], test_set[, -1], cl = training_set[, 1], k = 5)\nmc_test &lt;- table(test_set$Purchase, knn_model)\nprecision_test &lt;- (mc_test[1, 1] + mc_test[2, 2]) / (mc_test[1, 1] + mc_test[2, 2] + mc_test[1, 2] + mc_test[2, 1])\ncat(\"\\nPrecision k-NN en el conjunto de prueba con k =\", 5, \":\", precision_test)\n\n\nPrecision k-NN en el conjunto de prueba con k = 5 : 0.7242991\n\ncat(\"\\nError k-NN en el conjunto de prueba con k =\", 5, \":\", 1 - precision_test)\n\n\nError k-NN en el conjunto de prueba con k = 5 : 0.2757009\n\n\nEn este modelo, la regresión logística tiene como salida las probabilidades por lo que tenemos que convertirlas en las clases numericas que queremos.\n\nlistamodelos &lt;- list()\nlistaprecisiones &lt;- list()\n\nfor (i in 1:5) {\n  training_fold &lt;- training_set[-folds[[i]],]\n  test_fold &lt;- training_set[folds[[i]],]\n  clasificador &lt;- glm(Purchase ~ ., family = binomial, data = training_fold)\n  y_pred &lt;- predict(clasificador, type = 'response', newdata = test_fold)\n  y_pred &lt;- ifelse(y_pred &gt; 0.5, 1, 0)\n  y_pred &lt;- factor(y_pred, levels = c(\"0\", \"1\"), labels = c(\"MM\", \"CH\"))\n  mc &lt;- table(test_fold$Purchase, y_pred)\n  precision &lt;- (mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\n  listamodelos[[i]] &lt;- clasificador\n  listaprecisiones[[i]] &lt;- precision\n}\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nprecisionRegresionLogistica &lt;- mean(as.numeric(listaprecisiones))\ncat(\"\\nPrecision Regresion Logistica:\",precisionRegresionLogistica)\n\n\nPrecision Regresion Logistica: 0.823589\n\ncat(\"\\nError Regresion Logistica:\",1-precisionRegresionLogistica)\n\n\nError Regresion Logistica: 0.176411\n\nindice&lt;- which.max(listaprecisiones)\nmejormodelo &lt;- listamodelos[[indice]]\npredicciones &lt;- predict(mejormodelo, type = 'response', newdata = test_set)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\npredicciones &lt;- ifelse(predicciones &gt; 0.5, 1, 0)\npredicciones &lt;- factor(predicciones, levels = c(\"0\", \"1\"), labels = c(\"MM\", \"CH\"))\nmc &lt;- table(test_set$Purchase, predicciones)\nprecision &lt;-(mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\ncat(\"\\nPrecision Regresion Logistica:\",precision)\n\n\nPrecision Regresion Logistica: 0.8457944\n\ncat(\"\\nError Regresion Logistica:\",1-precision)\n\n\nError Regresion Logistica: 0.1542056\n\n\nA continuación para support vector machines, probamos con distintos núcleos y usamos cross=5 para realizar la validación cruzada. El hecho de calcular los valores apropiados para cada núcleo aumenta mucho el tiempo de computación.\n\nkernels &lt;- c(\"linear\", \"radial\", \"polynomial\", \"sigmoid\")\n\nfor (kernel in kernels) {\n  tobj &lt;- tune.svm(Purchase ~ ., data = training_set, gamma = 10^(-3:0), cost = 10^(0:2), kernel = kernel)\n  clasificador &lt;- svm(Purchase ~ ., data = training_set, gamma = tobj$best.parameters[[1]], cost = tobj$best.parameters[[2]],kernel = kernel, cross = 3, max_iter = 5)\n  \n  y_pred &lt;- predict(clasificador, test_set[, -1])\n  y_pred &lt;- ifelse(y_pred &gt; 0.5, 1, 0)\n  \n  true &lt;- test_set$Purchase\n  \n  mc &lt;- table(y_pred, true)\n  precisionSVM &lt;- (mc[1, 1] + mc[2, 2]) / (mc[1, 1] + mc[2, 2] + mc[1, 2] + mc[2, 1])\n  \n  cat(\"\\nKernel:\", kernel)\n  cat(\"\\nPrecision SVM:\", precisionSVM)\n  cat(\"\\nError SVM:\", 1 - precisionSVM)\n}\n\nWarning in cret$cresults * scale.factor: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\n\n\nKernel: linear\nPrecision SVM: 0.8271028\nError SVM: 0.1728972\n\n\nWarning in cret$cresults * scale.factor: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\n\n\nKernel: radial\nPrecision SVM: 0.8130841\nError SVM: 0.1869159\n\n\nWarning in cret$cresults * scale.factor: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\n\n\nKernel: polynomial\nPrecision SVM: 0.7803738\nError SVM: 0.2196262\n\n\nWarning in cret$cresults * scale.factor: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\n\n\nKernel: sigmoid\nPrecision SVM: 0.8224299\nError SVM: 0.1775701\n\n\nEl siguiente código entrena prueba los árboles de decisión.\n\nlistamodelos &lt;- list()\nlistaprecisiones &lt;- list()\n\nfor (i in 1:5) {\n  training_fold &lt;- training_set[-folds[[i]], ]\n  test_fold &lt;- training_set[folds[[i]], ]\n  training_fold$Purchase &lt;- as.factor(training_fold$Purchase)\n  test_fold$Purchase &lt;- as.factor(test_fold$Purchase)\n  clasificador &lt;- rpart(Purchase ~ ., data = training_fold)\n  y_pred &lt;- predict(clasificador, newdata = test_fold, type = 'class')\n  mc &lt;- table(test_fold$Purchase, y_pred)\n  precision &lt;- (mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\n  listamodelos[[i]] &lt;- clasificador\n  listaprecisiones[[i]] &lt;- precision\n}\n\nindice&lt;- which.max(listaprecisiones)\nmejormodelo &lt;- listamodelos[[indice]]\nprecisionDecisionTree &lt;- mean(as.numeric(listaprecisiones))\ncat(\"\\nPrecision media DecisionTree:\",precisionDecisionTree)\n\n\nPrecision media DecisionTree: 0.821243\n\ncat(\"\\nError medio DecisionTree:\",1-precisionDecisionTree)\n\n\nError medio DecisionTree: 0.178757\n\nmejormodelo &lt;- listamodelos[[indice]]\ntest_set$Purchase &lt;- as.factor(test_set$Purchase)\npredicciones &lt;- predict(mejormodelo, newdata = test_set, type = 'class')\nmc &lt;- table(test_set$Purchase, predicciones)\n\nprecision &lt;-(mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\ncat(\"\\nPrecision DecisionTree:\",precision)\n\n\nPrecision DecisionTree: 0.817757\n\ncat(\"\\nError DecisionTree:\",1-precision)\n\n\nError DecisionTree: 0.182243\n\n\nComo modelo avanzado, primero veremos los random forest.\n\nlistamodelos &lt;- list()\nlistaprecisiones &lt;- list()\n\nfor (i in 1:5) {\n  training_fold &lt;- training_set[-folds[[i]], ]\n  test_fold &lt;- training_set[folds[[i]], ]\n  training_fold$Purchase &lt;- as.factor(training_fold$Purchase)\n  test_fold$Purchase &lt;- as.factor(test_fold$Purchase)\n  clasificador &lt;- randomForest(Purchase ~ ., data = training_fold, ntree = 1000)\n  y_pred &lt;- predict(clasificador, newdata = test_fold)\n  mc &lt;- table(test_fold$Purchase, y_pred)\n  precision &lt;- (mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\n  listamodelos[[i]] &lt;- clasificador\n  listaprecisiones[[i]] &lt;- precision\n}\nprecisionRandomForest &lt;- mean(as.numeric(listaprecisiones))\n\ncat(\"\\nPrecision media RandomForest:\",precisionRandomForest)\n\n\nPrecision media RandomForest: 0.8212498\n\ncat(\"\\nError medio RandomForest:\",1-precisionRandomForest)\n\n\nError medio RandomForest: 0.1787502\n\nmejormodelo &lt;- listamodelos[[indice]]\ntest_set$Purchase &lt;- as.factor(test_set$Purchase)\npredicciones &lt;- predict(mejormodelo, newdata = test_set, type = 'class')\nmc &lt;- table(test_set$Purchase, predicciones)\n\nprecision &lt;-(mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\ncat(\"\\nPrecision RandomForest:\",precision)\n\n\nPrecision RandomForest: 0.7990654\n\ncat(\"\\nError RandomForest:\",1-precision)\n\n\nError RandomForest: 0.2009346\n\n\nFinalmente el último modelo será ADABOOST:\n\nlistamodelos &lt;- list()\nlistaprecisiones &lt;- list()\n\nfor (i in 1:5) {\n  training_fold &lt;- training_set[-folds[[i]], ]\n  test_fold &lt;- training_set[folds[[i]], ]\n  training_fold$Purchase &lt;- as.factor(training_fold$Purchase)\n  test_fold$Purchase &lt;- as.factor(test_fold$Purchase)\n  clasificador &lt;- boosting(Purchase ~ ., data = training_fold, boos=TRUE, mfinal=50)\n  y_pred &lt;- predict(clasificador, newdata = test_fold)$class\n  mc &lt;- table(test_fold$Purchase, y_pred)\n  precision &lt;- (mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\n  listamodelos[[i]] &lt;- clasificador\n  listaprecisiones[[i]] &lt;- precision\n}\nprecisionAdaBoost &lt;- mean(as.numeric(listaprecisiones))\n\ncat(\"\\nPrecision media AdaBoost:\",precisionAdaBoost)\n\n\nPrecision media AdaBoost: 0.8224126\n\ncat(\"\\nError medio AdaBoost:\",1-precisionAdaBoost)\n\n\nError medio AdaBoost: 0.1775874\n\nmejormodelo &lt;- listamodelos[[indice]]\ntest_set$Purchase &lt;- as.factor(test_set$Purchase)\npredicciones &lt;- predict(mejormodelo, newdata = test_set)$class\nmc &lt;- table(test_set$Purchase, predicciones)\n\nprecision &lt;-(mc[1,1] + mc[2,2]) / (mc[1,1] + mc[2,2] +mc[1,2] + mc[2,1])\ncat(\"\\nPrecision AdaBoost:\",precision)\n\n\nPrecision AdaBoost: 0.7476636\n\ncat(\"\\nError AdaBoost:\",1-precision)\n\n\nError AdaBoost: 0.2523364\n\n\nDespués de ver los resultados basados en la métrica eligida(precisión), eligiríamos la regresión logística pues es el modelo que menor error comete. Por otro lado también podríamos usar la discriminación lineal pues la diferencia entre los dos resultados es muy pequeña.\nA la horaa de implementar el modelo en la realidad la elección dependería de cómo se toman los datos y de la velocidad de resolución de modelo. A priori podemos pensar que LDA es más rápido por su sencillez pero puede dar algún problema con las correlaciones de las variables."
  },
  {
    "objectID": "TFG.html",
    "href": "TFG.html",
    "title": "6  TFG",
    "section": "",
    "text": "7 Objetivos\nUno de los problemas principales de la estadística moderna es aproximar densidades difíciles de calcular. Este problema es muy importante en la estadística bayesiana.\nLa situación en la inferencia clásica era la siguiente:\nAl contrario que en la inferencia clásica donde el parámetro \\(\\theta\\) estaba fijo, en la Inferencia Bayesiana \\(\\theta \\in \\Theta\\) es un vector aleatorio con distribución \\(\\pi(\\theta)\\). La distribución a posteriori está determinada por la densidad condicionada:\n\\[\nf(\\theta|x) = \\frac{f(x|\\theta)\\pi(\\theta)}{f(x)} = \\frac{f(x|\\theta)\\pi(\\theta)}{\\int_{\\Theta}\\pi(\\theta)f(x|\\theta)d\\theta}\n\\]\nMás adelante va a ser conveniente reservar la notación \\(\\theta\\) para otros parámetros y, por tanto, usaremos la notación habitual de la Inferencia Bayesiana:\n\\[\np(z|x) = \\frac{p(z,x)}{p(x)} = \\frac{p(z)p(x|z)}{\\int_{Z}p(x,z)dz} = \\frac{p(z)p(x|z)}{\\int_{Z}p(z)p(x|z)dz}\n\\]\nEn la inferencia variacional, los términos de la ecuación son:\nEn los modelos bayesianos, los parámetros van a jugar el papel de variables latentes. La inferencia en un modelo bayesiano equivale a condicionar por los datos y calcular la distribución a posteriori.\nEn muchos modelos bayesianos complejos, la distribución a posteriori no se puede calcular y se hace necesario recurrir a aproximación. Veamos un ejemplo de este problema.\nCon el objetivo de simplificar nuestro problema usaremos familia variacional de campo medio . La particularidad de esta familia es que las densidades de las componentes latentes son independientes entre sí. Un miembro genérico de la familia se escribiría como: \\[\\begin{equation}\n    q(z_{1:m})=\\prod_{j=1}^m q_j(z_j).\n\\end{equation}\\] Cada variable latente \\(z_j\\) esta gobernada por su propio factor variacional, la densidad \\(q_j(z_j)\\).\nEstos factores variacionales son elegidos para maximizar \\(E_q\\left[\\log p(z,x)\\right]-E_q\\left[\\log q(z)\\right]\\) en la optimización.\nElegir una familia variacional de campo medio nos permite recurrir a la actualización por coordenadas, un método más simple de implementar que el descenso de gradiente.\\ Supongamos que tenemos \\(z_{1:m}\\) variables latentes y \\(x_{1:n}\\) datos observados. La actualización de las densidades \\(q\\in\\mathcal{Q}\\) que maximiza la cota inferior ELBO es \\[\\begin{equation}\n    q_k^{*}(z_k)\\approx  \\exp(E_{-k}\\left[\\log p(z_k|z_{-k},x_{1:n} )\\right]).\n    \\end{equation}\\] La notación \\(-k\\) indica todos menos el k-ésimo.\nInput: un modelo \\(p(\\textbf{x,z})\\), un conjunto de datos \\(\\textbf{x}\\)\nOutput: Una densidad variacional \\(q(\\textbf{z})=\\prod_{j=1}^{m}q_j(z_j)\\)\nInicialización: \\(q(z)\\) inicial con sus factores variacionales \\(q_j(z_j)\\)\nMientras no converja el ELBO hacer:\nfin\nLa familia exponencial usual tiene la siguiente forma: \\[f(x|\\theta)=h(x)\\exp\\{\\eta(\\theta)T(x) - A(\\eta(\\theta))\\}.\\] Trabajar en la familia exponencial simplifica la inferencia variacional. Es más sencillo de computar el algoritmo y permite escalar la inferencia variacional a grandes cantidades de datos.\nAhora consideraremos un modelo genérico \\(p(z,x)\\) en el que cada condicional completo está en la familia exponencial: \\[\\begin{equation*}\n    p(z_j|z_{-j},x) = h(z_j)\\exp\\{\\eta_j(z_j,x)^Tz_j-a(\\eta_j(z_{-j},x))\\}.\n\\end{equation*}\\] Considerando la inferencia variacional de campo medio para esta familia y usando la expresión para la actualización de las densidades: \\[\\begin{equation*}\n    \\begin{split}\n        q_j^{*}(z_j) & \\approx \\exp\\{E_{-j}\\left[\\log(p(z_k|z_{-k},x_{1:n}))\\right]\\} \\\\\n        &\\approx h(z_j)\\exp\\{E_{-j}\\left[\\eta_j(z_j,x)^T\\right]z_j\\}.\n    \\end{split}\n    \\end{equation*}\\] Cada una de las actualizaciones está en la familia exponencial y la actualización se reduce a calcular la actualización de los parámetros.\nVeamos como son las actualizaciones en el ejemplo que vimos de la mezcla de gaussianas. Tenemos dos conjuntos de variables latentes: las asignaciones \\(c_i\\) y las medias \\(\\mu_k\\). Veamos de nuevo el modelo resumido : \\[\\mu_k \\sim \\mathbf{N}(0,\\sigma^2)  \\hspace{1cm} k=1,...,K \\] \\[ c_i \\sim multinomial(1/K,...,1/K ) \\hspace{1cm} i=1,...n \\] \\[x_i | c_i,\\mu \\sim \\mathbf{N}(c_i ^T \\mu,1) \\hspace{1cm} i=1,...n \\] La familia de campo medio para este caso es: \\[\\begin{equation*}\n    q(\\mu_{1:k},c_{1:n})=\\prod _{k=1}^K q(\\mu_k;m_k,s_k^2)\\prod _{i=1}^n q(c_i;\\varphi_{ik}).\n    \\end{equation*}\\] Las actualizaciones que maximizan la cota inferior son: \\[\\begin{equation*}\nq^{*}(c_i)\\approx \\exp \\left\\lbrace  m_kx_i-(m_k^2+s_k^2)/2\\right\\rbrace\n\\end{equation*}\\] \\[\\begin{equation*}\n    \\varphi_{ik}=\\frac{\\exp \\left\\lbrace  m_kx_i-(m_k^2+s_k^2)/2\\right\\rbrace}{\\sum_{k=1}^K\\exp \\left\\lbrace  m_kx_i-(m_k^2+s_k^2)/2\\right\\rbrace}\n\\end{equation*}\\] \\[\\begin{equation*}\n    q^{*}(\\mu_k) \\sim \\mathbf{N}(m_k,s_k^2)\n\\end{equation*}\\] \\[m_k=\\frac{\\sum_{i=1}^n\\varphi_{ik}x_i}{\\frac{1}{\\sigma^2} + \\sum_{i=1}^n\\varphi_{ik}}, \\hspace{1.5cm} s_k^2=\\left(\\frac{1}{\\sigma^2} + \\sum_{i=1}^n\\varphi_{ik}\\right)^{-1}\\]\nLa inferencia variacional estocástica es un método que nos permite escalar la inferencia variacional a grandes conjuntos de datos.\nEl método consistirá en:\nActualizaremos el valor del parámetro siguiendo ascenso de gradiente: \\[\\theta^{(t+1)}=\\theta^{(t)} + \\gamma_t \\cdot \\nabla_{\\theta}ELBO^{(t+1)}\\] Por otro lado:\\ \\[ELBO \\approx \\frac{1}{N}\\sum_{i=1}^N \\log p(x|z_i)p(z_i)-\\log q_{\\theta}(z_i).\\] De esta forma sustituimos los sumatorios previos."
  },
  {
    "objectID": "TFG.html#objetivos-1",
    "href": "TFG.html#objetivos-1",
    "title": "6  TFG",
    "section": "7.1 Objetivos",
    "text": "7.1 Objetivos\n\nPresentar la Inferencia Bayesiana y la Inferencia Variacional.\nDefinir la divergencia Kullback-Leibler y mostrar su relación con la inferencia variacional.\nExponer un método computacional factible para el cálculo aproximado de la distribución a posteriori usando la actualización por coordenadas.\nMostrar una forma de escalar nuestro método para conjuntos de datos masivos."
  },
  {
    "objectID": "TFG.html#ejemplo-motivador",
    "href": "TFG.html#ejemplo-motivador",
    "title": "6  TFG",
    "section": "8.1 Ejemplo motivador",
    "text": "8.1 Ejemplo motivador\nConsideramos el siguiente modelo : \\[\\mu_k \\sim \\mathbf{N}(0,\\sigma^2)  \\hspace{1cm} k=1,...,K \\] \\[ c_i \\sim multinomial(1/K,...,1/K ) \\hspace{1cm} i=1,...n \\] \\[x_i | c_i,\\mu \\sim \\mathbf{N}(c_i ^T \\mu,1) \\hspace{1cm} i=1,...n \\] Entonces la densidad conjunta se escribe como: \\[p(\\mu_{1:K},c_{1:n},x_{1:n})=\\prod_{l=1}^K p(\\mu_l)\\prod_{i=1}^{n}p(c_i)p(x_i|c_i,\\mu_{1:K}).\\] Y marginalizando sobre las variables latentes tenemos que: \\[p(x_{1:n})=\\int \\prod_{l=1}^{K}p(\\mu_l)\\prod_{i=1}^n\\sum_{c_i}p(c_i)p(x_i|c_i,\\mu_{1:K})d\\mu.\\]\nLuego la densidad a posteriori es : \\[p(\\mu_{1:K},c_{1:n}|x_{1:n})=\\frac{\\prod_{l=1}^K p(\\mu_l)\\prod_{i=1}^{n}p(c_i)p(x_i|c_i,\\mu_{1:K})}\n{\\int \\prod_{l=1}^Kp(\\mu_l)\\prod_{i=1}^n\\sum_{c_i}p(c_i)p(x_i|c_i,\\mu_{1:K})d\\mu}.\\]\nEl integrando no contiene factores separados para cada \\(\\mu_k\\), cada \\(\\mu_k\\) aparece en los n factores del integrando. De este modo, la integral no se reduce a un producto de n integrales unidimensionales sobre los \\(\\mu_k's\\).\nAlternativamente, podemos escribir la densidad marginal como una suma de todas las posibles configuraciones de c: \\[p(x_{1:n})=\\sum_{c_i}p(c_i) \\int \\prod_{l=1}^Kp(\\mu_l)\\prod_{i=1}^{n}p(x_i|c_i,\\mu_{1:K})d\\mu.\\]\nAhora, podemos calcular cada miembro de la integral, pero hay \\(K^n\\) componentes. Por tanto, calcular la densidad marginal no es posible."
  },
  {
    "objectID": "TFG.html#soluciones",
    "href": "TFG.html#soluciones",
    "title": "6  TFG",
    "section": "8.2 Soluciones",
    "text": "8.2 Soluciones\nDurante décadas, la aproximación se resolvía mediante MCMC: métodos de Montecarlo basados en Cadenas de Markov.\n\nSe construye una cadena de Markov ergódica en \\(z\\) cuya distribución estacionaria es la distribución a posteriori.\n\nAsintóticamente MCMC produce muestras exactas a la distribución a posteriori. La inferencia variacional nos dará una respuesta menos exacta pero más rapida.\nLa inferencia variacional resuelve el mismo problema también mediante optimización.\n\nSeleccionamos una familia de densidades aproximantes \\(\\mathcal{Q}\\) sobre las variables latentes.\nTratamos de encontrar el miembro de la familia que minimice la divergencia de Kullback-Leibler(KL) a la distribución a posteriori. \\[\\begin{equation}\\label{problema}\nq^{*}= \\arg\\min_{q\\in\\mathcal{Q}} KL (q\\ ||\\ p (\\cdot|x)).\n\\end{equation}\\]\n\nUna de las claves de la inferencia variacional es elegir correctamente la familia \\(\\mathcal{Q}\\) para encontrar una densidad cercana a \\(p(z|x)\\) pero lo suficientemente sencilla para no complicar el proceso de optimización."
  },
  {
    "objectID": "TFG.html#definición",
    "href": "TFG.html#definición",
    "title": "6  TFG",
    "section": "9.1 Definición",
    "text": "9.1 Definición\nSean P y Q probabilidades definidas en el mismo espacio. Si \\(P\\ll Q\\) entonces: \\[\\begin{equation}\n    D_{KL}(P,Q)= \\int \\log \\frac{dP}{dQ}dP.\n\\end{equation}\\] Si P no es absolutamente continua respecto de Q entonces \\(D_{KL}(P,Q)= +\\infty\\)."
  },
  {
    "objectID": "TFG.html#proposición",
    "href": "TFG.html#proposición",
    "title": "6  TFG",
    "section": "9.2 Proposición",
    "text": "9.2 Proposición\nSi P y Q son probabilidades en el mismo espacio entonces \\(D_{KL}(P,Q) \\geq 0\\). La igualdad se da si y solo si \\(P=Q\\)."
  },
  {
    "objectID": "TFG.html#la-divergencia-de-kullback---leiber-y-la-inferencia-variacional",
    "href": "TFG.html#la-divergencia-de-kullback---leiber-y-la-inferencia-variacional",
    "title": "6  TFG",
    "section": "La divergencia de Kullback - Leiber y la inferencia variacional",
    "text": "La divergencia de Kullback - Leiber y la inferencia variacional\nA la vista de \\[\\begin{equation}\n    q^{*}= \\arg\\min_{q\\in\\mathcal{Q}} KL (q\\ ||\\ p (\\cdot|x)).\n    \\end{equation}\\] puede parecer sorprendente que podamos conocer la divergencia KL entre dos elementos de los cuales hay uno que no podemos calcular.\nA continuación veremos como resolver este problema \\[\\begin{equation*}\n    \\begin{split}\nD_{KL}(q\\ || \\ p (\\cdot|x)) &= \\int q(z)\\log\\frac{q(z)}{p(z|x)}dz\\\\\n&=E_q\\left[\\log q(z)\\right] - E_q\\left[\\log p(z,x)\\right] + \\log p(x).\n    \\end{split}\n    \\end{equation*}\\] Como la divergencia es positiva tenemos una cota para el logaritmo de la densidad marginal o evidencia."
  },
  {
    "objectID": "TFG.html#definición-1",
    "href": "TFG.html#definición-1",
    "title": "6  TFG",
    "section": "9.3 Definición",
    "text": "9.3 Definición\nDicha cota inferior se conoce en inglés como ELBO(Evidence Lower Bound). \\[\\begin{equation}\\label{ELBO}\n    ELBO:= E_q\\left[\\log p(z,x)\\right]-E_q\\left[\\log q(z)\\right] .\n\\end{equation}\\] Entonces la divergencia se puede escribir como: \\[D_{KL}(q\\ || \\ p (\\cdot|x)) = \\log p(x) - ELBO.\\] Por tanto para minimizar la divergencia, como no podemos calcular \\(\\log p(x)\\), maximizaremos el ELBO: $E_q-E_q$."
  },
  {
    "objectID": "TFG.html#mezcla-de-gaussianas-en-una-dimensión",
    "href": "TFG.html#mezcla-de-gaussianas-en-una-dimensión",
    "title": "6  TFG",
    "section": "13.1 Mezcla de gaussianas en una dimensión",
    "text": "13.1 Mezcla de gaussianas en una dimensión\nRepresentaremos el ejemplo motivador y lo resolveremos usando la inferencai variacional.\n\nsuppressMessages({\n    suppressWarnings({\nlibrary(mvtnorm) #Para las normales multivariantes\nlibrary(extraDistr) #Para la distribucion multinomial\nlibrary(pracma) #Para integrar\nlibrary(matlib) #Para resolver ecuaciones\nlibrary(ggplot2) #Para dibujar\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(ggforce)\n    })})\n\nUsaremos los siguientes hiperparámetros\n\nsigma = 3 #Varianza de mu\nK = 5 #Numero de clusters o grupos\nn = 1000 #Numero de observaciones\n\n\nmu = rnorm(K,mean = 0, sd=sigma) # Medias\nmu = sort(mu) #Para despues dibujarlas y compararlas bien\ncs = rcat(n, rep(1/K,K)) # Asignaciones\nx  = rnorm(n,mean=mu[cs], sd=1) #Generamos los datos\n\n#Dibujamos los datos\ndf = data.frame(x=x, mu=as.factor(cs))\nggplot(df, aes(x=x,color=mu, fill=mu)) +geom_histogram(alpha=0.5)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n#Funcion para calcular el ELBO#\nELBO=function(mk,sk2,phis){\n  t = sk2+mk^2\n  a = -(1/(2*sigma^2))*sum(t)\n  b = 2*sum(sweep(sweep(phis,MARGIN = 2,mk,'*'),MARGIN = 1,x,'*'))-0.5*sum(sweep(phis,MARGIN = 2,t,'*' ))\n  c = -0.5*sum(log(2*pi*sk2))\n  d = sum(phis*log(phis))\n  return(a+b+c+d)\n}\n#creamos una lista de data.frames para poder dibujar las distintas iteraciones del ELBO\nlista_df &lt;- list()\n\n#Realizamos el algoritmo CAVI 5 veces para comparar su convergencia.\nfor (z in 1:5){\n  ### Algoritmo CAVI ###\n  mk= rnorm(K) #Generamos la muestra inicial\n  sk2=rgamma(K,5) #Varianzas aleatorias positivas\n  phis=rdirichlet(n,c(1,1,1,1,1)) #Asignaciones aleatorias\n  \n  \n  iter=30 #Iteraciones\n  elbos=rep(NA,iter+1) #Inicializamos el ELBO\n  elbos[1]=ELBO(mk,sk2,phis)\n  #Con las ecuaciones obtenidas anteriormente realizamos hasta la convergencia del ELBO\n  #o hasta llegar al numero de iteraciones\n  for (i in 1:iter){\n    phis.new=matrix(nrow=n,ncol = K)\n    #Actualizamos las asignaciones\n    for (j in 1:n){\n      phis.new[j,]=exp(x[j]*mk-0.5*(sk2+mk^2))\n      phis.new = phis.new/rowSums(phis.new)\n    }\n    phis=phis.new\n    \n    mk.new=rep(NA,K)\n    sk2.new=rep(NA,K)\n    #Actualizamos los componentes de la mezcla\n    for (k in 1:K){\n      sk2.new[k] = 1/(1/sigma^2+sum(phis[,k]))\n      mk.new[k]  = sk2.new[k]*sum(phis[,k]*x)\n    }\n    \n    sk2=sk2.new\n    mk=mk.new\n    #Calculamos el ELBO y comprobamos su convergencia.\n    elbos[i+1]=ELBO(mk,sk2,phis)\n    if(z==1){\n      cat(\"Iteracion: \",i, \"Diferencia de ELBO: \",abs(elbos[i+1]-elbos[i]),\"\\n\")\n       }\n    if (abs(elbos[i+1]-elbos[i])&lt;0.1) break\n  }\n  #Anadimos los datos a la lista de los data.frame\n  lista_df[z] &lt;- list(data.frame(Iter = seq(1, i, 1), ELBO = elbos[1:i]))\n  \n}\n\nIteracion:  1 Diferencia de ELBO:  11206.89 \nIteracion:  2 Diferencia de ELBO:  1344.463 \nIteracion:  3 Diferencia de ELBO:  303.6344 \nIteracion:  4 Diferencia de ELBO:  73.54252 \nIteracion:  5 Diferencia de ELBO:  21.17064 \nIteracion:  6 Diferencia de ELBO:  7.611186 \nIteracion:  7 Diferencia de ELBO:  3.78807 \nIteracion:  8 Diferencia de ELBO:  2.506851 \nIteracion:  9 Diferencia de ELBO:  1.928397 \nIteracion:  10 Diferencia de ELBO:  1.587544 \nIteracion:  11 Diferencia de ELBO:  1.360781 \nIteracion:  12 Diferencia de ELBO:  1.204313 \nIteracion:  13 Diferencia de ELBO:  1.093541 \nIteracion:  14 Diferencia de ELBO:  1.01127 \nIteracion:  15 Diferencia de ELBO:  0.945631 \nIteracion:  16 Diferencia de ELBO:  0.8890867 \nIteracion:  17 Diferencia de ELBO:  0.837324 \nIteracion:  18 Diferencia de ELBO:  0.7881786 \nIteracion:  19 Diferencia de ELBO:  0.7407767 \nIteracion:  20 Diferencia de ELBO:  0.6949343 \nIteracion:  21 Diferencia de ELBO:  0.6507811 \nIteracion:  22 Diferencia de ELBO:  0.6085442 \nIteracion:  23 Diferencia de ELBO:  0.5684382 \nIteracion:  24 Diferencia de ELBO:  0.530618 \nIteracion:  25 Diferencia de ELBO:  0.4951666 \nIteracion:  26 Diferencia de ELBO:  0.4621 \nIteracion:  27 Diferencia de ELBO:  0.4313795 \nIteracion:  28 Diferencia de ELBO:  0.4029257 \nIteracion:  29 Diferencia de ELBO:  0.3766308 \nIteracion:  30 Diferencia de ELBO:  0.3523696 \n\n\nGeneramos un data.frame completo y dibujamos el ELBO y el numero de iteraciones.\n\ndf_completo &lt;- bind_rows(lista_df, .id = \"ID\")\nggplot(data = df_completo, aes(x = Iter, y = ELBO, color = ID)) +geom_line(linewidth=1.5) +\n  labs(title=\"Convergencia del ELBO \", x=\"Iteraciones\", y=\"ELBO\")\n\n\n\n\nOrdenamos las medias y las comparamos con las originales\n\nmk=sort(mk);\nmk\n\n[1] -3.7376391 -3.1900522 -2.3752398 -1.5089553 -0.2703252\n\nmu\n\n[1] -3.6680851 -3.2648079 -2.6094741 -1.7623829 -0.4297341\n\n\nDibujamos los datos con las medias que hemos calculado previamente.\n\nggplot(df,aes(x=x,color=mu,fill=mu)) + geom_histogram(alpha=0.5)+ \n  geom_vline(data=data.frame(x=mk),aes(xintercept=x, color=as.factor(c(1,2,3,4,5))),linetype=\"dashed\",size=1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nComprobemos como generamos los datos con las medias obtenidas\n\ncs2 = rcat(n, rep(1/K,K)) # Asignaciones\nxnew  = rnorm(n,mean=mk[cs2], sd=1) #Generamos los datos,\n\ndf2 = data.frame(x=xnew, mu=as.factor(cs2))\nggplot(df2, aes(x = x, color = mu, fill = mu)) + geom_histogram(alpha = 0.5)       \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "TFG.html#mezcla-de-gaussianas-en-dos-dimensiones",
    "href": "TFG.html#mezcla-de-gaussianas-en-dos-dimensiones",
    "title": "6  TFG",
    "section": "13.2 Mezcla de gaussianas en dos dimensiones",
    "text": "13.2 Mezcla de gaussianas en dos dimensiones\nLos hiperparámetros en este caso serán:\n\nsigma = 4 #Varianza de mu\nK = 5 #Numero de clusters o grupos\nn = 1000 #Numero de observaciones\n\nAdaptaremos nuestro algortimo para los datos multivariantes.\n\n#Generamos los datos que ahora son datos multivariantes.\nI=diag(c(1,1))\nmu = rmvnorm(K, mean=c(0,0), sigma = sigma^2*I) \ncs=rcat(n, rep(1/K,K))\nx=mu[cs,] + rmvnorm(n, mean=c(0,0),sigma = I)\n\n#Funcion para ordenar las medias segun su norma, nos servira para compararlas despues\nordenarmedias = function(medias){\n  normas = apply(medias, 1, function(x) sqrt(sum(x^2)))\n  medias = medias[order(normas),]\n  return(medias)\n}\n\n\n#Dibujemos los datos\ndf = data.frame(x = x, mu = as.factor(cs))\nggplot(df, aes(x = x[,1], y = x[,2], color = mu, fill = mu)) + geom_point()\n\n\n\n###Algoritmo CAVI###\n#Generamos los datos iniciales pero teniendo en cuenta que ahora son multivariantes,\n#por tanto tomamos I\nmk = rmvnorm(K, mean = c(0,0), sigma = I)\nsk2 = rgamma(K, 5)\nphis = rdirichlet(n, c(1,1,1,1,1))\n\n#Funcion para dibujar los datos y las medias\nplotClusters = function(){\n  ggplot(df, aes(x = x[,1], y = x[,2], color = mu, fill = mu)) + \n    geom_point(alpha = 0.3) +\n    geom_point(data = data.frame(x1 = mk[,1], x2 = mk[,2], mu = as.factor(c(3,5,4,2,1))),\n               aes(x = x1, y = x2, color = mu), size = 3, colour = \"black\") +\n    geom_point(data = data.frame(x1 = mk[,1], x2 = mk[,2], mu = as.factor(c(3,5,4,2,1))),\n               aes(x = x1, y = x2, color = mu), size = 2) + \n    geom_circle(data = data.frame(x1 = mk[,1], x2 = mk[,2], mu = as.factor(c(3,5,4,2,1))), \n                aes(x0 = x1, y0 = x2, r = sigma/2, fill = mu, x = x1, y = x2), color = \"black\",alpha = 0.2)\n}\n\nplotClusters()\n\n\n\n#Numero de iteraciones\niter = 50\n#Realizamos el algoritmo hasta llegar a las 30 iteraciones\n#Notemos que ahora no son escalares y por tanto tenemos que tener cuidado al multiplicar vectores.\nfor (i in 1:iter){\n  phis.new = matrix(nrow = n, ncol = K)\n  \n  for (j in 1:n){\n    for (k in 1:K){\n      phis.new[j,k] = exp(t(x[j,]) %*% mk[k,] - 0.5 * (2 * sk2[k] + t(mk[k,])\n                                                       %*% mk[k,]))\n    }\n    phis.new = phis.new / rowSums(phis.new)\n  }\n  phis = phis.new\n  \n  mk.new = matrix(rep(NA, K * 2), ncol = 2)\n  sk2.new = rep(NA, K)\n  \n  for (k in 1:K){\n    sk2.new[k] = 1 / (1 / sigma^2 + sum(phis[,k]))\n    mk.new[k,] = sk2.new[k] * colSums(phis[,k] * x)\n  }\n  \n  sk2 = sk2.new\n  mk = mk.new\n  \n  #Para guardar la evolucion tras 5 iteraciones\n  if (i == 5){\n    df_iter5 &lt;- df\n    df_iter5$mu &lt;- as.factor(colSums(phis == max.col(phis)))\n    mk_iter5 &lt;- ordenarmedias(mk)\n  }\n  #Para guardar la evolucion tras 20 iteraciones\n  \n  if (i == 20){\n    df_iter20 &lt;- df\n    df_iter20$mu &lt;- as.factor(colSums(phis == max.col(phis)))\n    mk_iter20 &lt;- ordenarmedias(mk)\n  }\n}\n\nPara dibujar como estan las medias tras 5 iteraciones\n\nggplot(df_iter5, aes(x = x[,1], y = x[,2], color = mu, fill = mu)) +\n  geom_point(alpha = 0.3) +\n  geom_point(data = data.frame(x1 = mk_iter5[,1], x2 = mk_iter5[,2], mu = as.factor(c(4,1,3,2,5))),\n             aes(x = x1, y = x2, color = mu), size = 3, colour = \"black\") +\n  geom_point(data = data.frame(x1 = mk_iter5[,1], x2 = mk_iter5[,2], mu = as.factor(c(4,1,3,2,5))),\n             aes(x = x1, y = x2, color = mu), size = 2) +\n  geom_circle(data = data.frame(x1 = mk_iter5[,1], x2 = mk_iter5[,2], mu = as.factor(c(4,1,3,2,5))),\n              aes(x0 = x1, y0 = x2, r = sigma/2, fill = mu, x = x1, y = x2), color = \"black\", alpha = 0.2)\n\n\n\n\nPara dibujar como estan las medias tras 20 iteraciones\n\nggplot(df_iter20, aes(x = x[,1], y = x[,2], color = mu, fill = mu)) +\n  geom_point(alpha = 0.3) +\n  geom_point(data = data.frame(x1 = mk_iter20[,1], x2 = mk_iter20[,2], mu = as.factor(c(4,1,3,2,5))),\n             aes(x = x1, y = x2, color = mu), size = 3, colour = \"black\") +\n  geom_point(data = data.frame(x1 = mk_iter20[,1], x2 = mk_iter20[,2], mu = as.factor(c(4,1,3,2,5))),\n             aes(x = x1, y = x2, color = mu), size = 2) +\n  geom_circle(data = data.frame(x1 = mk_iter20[,1], x2 = mk_iter20[,2], mu = as.factor(c(4,1,3,2,5))),\n              aes(x0 = x1, y0 = x2, r = sigma/2, fill = mu, x = x1, y = x2), color = \"black\", alpha = 0.2)\n\n\n\nplotClusters()\n\n\n\n\nComparamos el resultado con el algoritmo kmeans Dibujamos donde se encuentran las medias artificiales, las reales y las kmedias.\n\nkmeans_result &lt;- kmeans(x, centers = 5)\nplot(mk,col=\"red\",xlab = \"x1\",ylab = \"x2\");\npoints(mu,col=\"blue\")\npoints(kmeans_result$centers,col=\"green\")\n\n\n\n\nEn el siguiente enlace podemos ver la evolución de las medias para un ejemplo en específico.\n\nVer la evolución de las medias"
  },
  {
    "objectID": "TFG.html#análisis-de-imágenes",
    "href": "TFG.html#análisis-de-imágenes",
    "title": "6  TFG",
    "section": "13.3 Análisis de imágenes",
    "text": "13.3 Análisis de imágenes\nPor último, intentaremos agrupar unas imágenes según su frecuencia de colores. Las imágenes provienen de kaggle y son 8000 imágenes de bosques, playas, desiertos y glaciares.\nUtilizando nuestro algoritmo podemos estudiar los histogramas concatenados de rojo, verde y azul de las imágenes.\nEstos histogramas serán nuestros nuevos datos multivariantes a los que aplicaremos el modelo. \nTras estandarizar los datos y aplicar el modelo obtenemos los siguientes 4 histogramas tipo.\n\n\n\nLos 4 histogramas obtenidos"
  }
]